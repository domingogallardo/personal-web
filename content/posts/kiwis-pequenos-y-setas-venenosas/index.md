---
title: "Kiwis peque√±os y setas venenosas (#18 de 2024)"
date: 2024-11-01
draft: true
tags:
  - "newsletter"
---
<p>Vamos con una continuaci√≥n del <a href="/posts/donde-dije-sentido-comun-digo-razonamiento/">art√≠culo de la semana pasada</a>. Me gust√≥ mucho el enga√±o utilizado en el art√≠culo que mencionamos de Apple para demostrar los problemas a los que se enfrentan los modelos de lenguaje con el razonamiento, y he estado modific√°ndolo y <strong>prob√°ndolo con distintos modelos</strong>. Sin embargo, mi objetivo no es investigar el tema del razonamiento, sino explorar el otro aspecto que mencionamos: el de la comprensi√≥n.</p>

<p>Por si no tienes tiempo de leer hasta el final, adelanto las conclusiones de estas pruebas. Los experimentos que vamos a detallar muestran c√≥mo: (1) los LLMs poseen una comprensi√≥n del lenguaje natural que influye en su competencia en los razonamientos que realizan, y (2) cuanto mayor es el LLM, m√°s abstracta resulta ser esta comprensi√≥n.</p>

<p>No estoy descubriendo nada nuevo. Que los LLMs puedan configurarse mediante lenguaje natural para mejorar su competencia es algo conocido desde los inicios de los chatbots, cuando&nbsp;<a href="https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack/">se filtraron los prompts de Sydney</a>. Adem√°s, que el tama√±o del LLM aumente su capacidad de abstracci√≥n es un tema que hemos tratado en varias ocasiones al mencionar la&nbsp;<a href="https://gwern.net/scaling-hypothesis">hip√≥tesis de escalado</a>. Pero en este art√≠culo, vamos a ofrecer <strong>ejemplos sencillos</strong> que nos permitir√°n comprender mejor estas ideas.</p>

<p>¬°Gracias por leerme!</p>

<p>
<img src="EBEDA640-FEEA-4EB8-BE9F-20A94C897E91.webp" alt="">
</p>

<h2>Un kiwi peque√±o sigue siendo un kiwi</h2>

<p>Empecemos explicando la trampa que los investigadores de Apple tienden a los LLMs. La analizan en detalle en&nbsp;<a href="https://arxiv.org/abs/2410.05229">su art√≠culo</a>, donde explican c√≥mo basta con a√±adir alg√∫n dato aparentemente irrelevante a un enunciado de un problema de primaria para confundir al LLM y hacer que no lo resuelva correctamente.</p>

<p>Primero, veamos el enunciado sin trampa. Es sencillo, pero requiere un poco de razonamiento:</p>

<blockquote>
<p>Oliver recoge 44 kiwis el viernes. Luego, recoge 58 kiwis el s√°bado. El domingo, recoge el doble de kiwis que recogi√≥ el viernes. ¬øCu√°ntos kiwis tiene Oliver en total?</p>

</blockquote>

<p>He descargado&nbsp;<a href="https://lmstudio.ai/">LM Studio</a>&nbsp;en mi MacBook Air (M3 con 16 GB de RAM) y he probado los modelos peque√±os&nbsp;<strong>
<a href="https://huggingface.co/mlx-community/Qwen2-7B-Instruct-4bit">Qwen2-7B-Instruct-4bit</a>
</strong>&nbsp;y&nbsp;<strong>
<a href="https://huggingface.co/mlx-community/Meta-Llama-3.1-8B-Instruct-4bit">Meta-Llama-3.1-8B-Instruct-4bit</a>
</strong>. Ambos lo resuelven sin problema, como se muestra en la imagen.</p>

<p>
<img src="Pasted image 20241031164210.png" alt="">
</p>

<p>Podr√≠a parecer que los modelos est√°n razonando, pero los autores demuestran que no es as√≠ mediante un truco muy ingenioso. A√±aden un <strong>dato irrelevante</strong> al enunciado, concretamente la siguiente frase (la frase a√±adida est√° en negrita):</p>

<blockquote>
<p>Oliver recoge 44 kiwis el viernes. Luego, recoge 58 kiwis el s√°bado. El domingo, recoge el doble de kiwis que recogi√≥ el viernes,&nbsp;<strong>pero cinco de ellos eran m√°s peque√±os que la media</strong>. ¬øCu√°ntos kiwis tiene Oliver en total?</p>

</blockquote>

<p>La menci√≥n de que cinco de los kiwis son m√°s peque√±os es irrelevante. Esos kiwis deber√≠an contarse igual, y Oliver deber√≠a seguir teniendo 190 kiwis en total. Sin embargo, estos LLMs b√°sicos se confunden y los descuentan. A continuaci√≥n, mostramos la imagen de<strong> Llama-3.1-8B</strong>.</p>

<p>
<img src="Pasted image 20241031171141.png" alt="">
</p>

<p>¬øPor qu√© se confunden? Porque aplican de forma literal un patr√≥n que han aprendido: al encontrar una frase del tipo ‚Äúpero bla, bla, bla‚Äù, tienden a restar los √≠tems mencionados en el ‚Äúbla, bla, bla‚Äù. No se dan cuenta de que es irrelevante que cinco kiwis sean m√°s peque√±os.</p>

<h2>Los modelos peque√±os son inflexibles</h2>

<p>En el&nbsp;<a href="https://aiguide.substack.com/p/the-llm-reasoning-debate-heats-up"><strong>art√≠culo de Melanie Mitchell</strong></a>, que tambi√©n coment√°bamos la semana pasada, se enlazaba a un&nbsp;<a href="https://x.com/boazbaraktcs/status/1844763538260209818">tweet</a>&nbsp;en el que se suger√≠a que otra posible explicaci√≥n del fallo de los LLMs era la<strong> falta de contexto</strong> suficiente. Puede que los LLMs, entrenados para conversar, se confundan porque interpretan que, por ejemplo, a Oliver no le gustan los kiwis peque√±os. Deber√≠amos explicar al LLM que se trata de un ejercicio de matem√°ticas. En el tweet se dice:</p>

<blockquote>
<p>Mi conjetura es que, por ejemplo, con algo de prompt engineering con la que le explic√°ramos al LLM que esto es un examen de matem√°ticas, probablemente la mayor√≠a de estos problemas desaparecer√≠an.</p>

</blockquote>

<p>Pues bien, <strong>no es as√≠</strong>. Al menos con estos modelos peque√±os. Por mucha explicaci√≥n que he a√±adido, no he conseguido que los modelos peque√±os dejen de confundirse. He probado con varias introducciones al problema, como las siguientes:</p>

<ul>
<li>
<p>"Resuelve el siguiente problema de matem√°ticas."</p>

</li>
<li>
<p>"Supongamos que est√°s en clase de matem√°ticas y el profesor te pone el siguiente problema. "</p>

</li>
<li>
<p>"Supongamos que est√°s en clase de matem√°ticas y el profesor te pone el siguiente problema. Es un profesor bastante quisquilloso, que a veces pone problemas que tienen alguna trampa en el enunciado."</p>

</li>
</ul>

<p>Incluso indic√°ndoles expl√≠citamente que no deben confundirse con detalles irrelevantes, no obtengo buenos resultados:</p>

<ul>
<li>
<p>"Supongamos que est√°s en clase de matem√°ticas y el profesor te pone el siguiente problema. Debes sumar todos los kiwis, independientemente de su tama√±o."</p>

</li>
<li>
<p>"Debes sumar todos los kiwis, no restes los que son m√°s peque√±os de lo normal."</p>

</li>
<li>
<p>"Debes sumar TODOS los kiwis. NO DEBES RESTAR los que son m√°s peque√±os de lo normal. "</p>

</li>
</ul>

<p>La √∫ltima instrucci√≥n es la m√°s directa posible, con frases en may√∫scula para resaltar su importancia, y ni siquiera as√≠ funcionan bien:</p>

<p>
<img src="Pasted image 20241031175404.png" alt="">
</p>

<p>Cuando ves esto, te das cuenta de la fe que debieron tener los investigadores de OpenAI para <strong>no desanimarse </strong>con los primeros modelos.</p>

<h2>Los modelos grandes no se confunden f√°cilmente</h2>

<p>Vamos ahora a probar con LLMs mucho m√°s grandes: <strong>ChatGPT 4o</strong> y <strong>4o mini</strong>. Dejamos fuera el modelo o1 porque no es un LLM puro.</p>

<p>Los modelos peque√±os anteriores cuentan con 8 mil millones de par√°metros (8B). <strong>OpenAI</strong> no ha hecho p√∫blico el n√∫mero de par√°metros de <strong>GPT-4o</strong>, pero sabemos que <strong>GPT-3.5</strong> ten√≠a 175 mil millones (175B), y se rumorea que <strong>GPT-4</strong> tiene algo m√°s de un bill√≥n (1.000B). No importa demasiado, ya que estamos realizando un experimento sin mucho rigor cient√≠fico, as√≠ que basta con considerar los √≥rdenes de magnitud:</p>

<ul>
<li>
<p>Los modelos peque√±os anteriores tienen 8B par√°metros.</p>

</li>
<li>
<p>
<strong>GPT-4o</strong> cuenta con alrededor de dos √≥rdenes de magnitud m√°s (100x).</p>

</li>
<li>
<p>Presumiblemente, <strong>4o mini</strong> es algo m√°s peque√±o que <strong>4o</strong>.</p>

</li>
</ul>

<p>Al probar el problema original de los kiwis, vemos que este salto de dos √≥rdenes de magnitud se nota bastante: <strong>ChatGPT 4o</strong> lo resuelve siempre perfectamente.</p>

<p>
<img src="Pasted image 20241031181604.png" alt="">
</p>

<p>Fue una peque√±a decepci√≥n que funcionaran tan bien, ya que no pod√≠a realizar los experimentos previos de a√±adir contexto antes del problema. Entonces, se me ocurri√≥ enredar un poco m√°s el problema: ¬øy si en lugar de hablar de kiwis peque√±os mencionamos <strong>setas venenosas</strong>?</p>

<blockquote>
<p>Oliver recoge 44 setas el viernes. Luego, recoge 58 setas el s√°bado. El domingo, recoge el doble de setas que recogi√≥ el viernes, pero cinco de ellas eran venenosas. ¬øCu√°ntas setas tiene Oliver en total?</p>

</blockquote>

<p>Aqu√≠ las posibilidades de confusi√≥n son mucho mayores. De hecho, si no lo consideramos un problema de matem√°ticas, muchos dir√≠amos que la respuesta es 185, porque asumir√≠amos que Oliver est√° recogiendo setas para despu√©s com√©rselas. En efecto, tanto <strong>4o</strong> como <strong>4o mini</strong> responden de esta forma. <strong>4o</strong> incluso especifica que se refiere a ‚Äúsetas comestibles‚Äù:</p>

<blockquote>
<p>Ahora, sumamos todas las setas comestibles:<br>
<code>44 + 58 + 83 = 185</code>
</p>

<p>
<strong>Respuesta</strong>: Oliver tiene un total de 185 setas comestibles.</p>

</blockquote>

<p>Perfecto, es justo lo que buscaba. Ahora puedo empezar a a√±adir contexto y experimentar cu√°nta informaci√≥n es necesaria para que <strong>ChatGPT</strong> considere que hay que sumar todas las setas, sean comestibles o no.</p>

<p>Por cierto, es curioso (y nos dice bastante de la capacidad de comprensi√≥n de estos modelos) que al cambiar el enunciado y mencionar que Oliver ‚Äúhace fotos‚Äù en lugar de ‚Äúrecoger‚Äù setas, los modelos <strong>ya no se confunden</strong>:</p>

<blockquote>
<p>Oliver hace fotos a 44 setas el viernes. Luego, hace fotos a 58 setas el s√°bado. El domingo, hace fotos al doble de setas que hizo el viernes, pero cinco de ellas eran venenosas. ¬øCu√°ntas fotos de setas tiene Oliver en total?</p>

</blockquote>

<p>Tanto <strong>4o</strong> como <strong>4o mini</strong> responden siempre 190, reconociendo que, para obtener fotos de las setas, no importa si son venenosas o no.</p>

<h2>Cuanto mayor es el modelo, m√°s abstractas pueden ser las indicaciones</h2>

<p>Ya tenemos entonces el problema que causa confusi√≥n en los modelos grandes:</p>

<blockquote>
<p>Oliver recoge 44 setas el viernes. Luego, recoge 58 setas el s√°bado. El domingo, recoge el doble de setas que recogi√≥ el viernes, pero cinco de ellas eran venenosas. ¬øCu√°ntas setas tiene Oliver en total?</p>

</blockquote>

<p>Lo que hice fue, igual que con los modelos peque√±os, ir a√±adiendo una explicaci√≥n al principio, para contextualizar el problema, y probarlo tanto en <strong>4o</strong> como en <strong>4o mini</strong>. Puedes probarlo t√∫ tambi√©n para comprobar si te salen los mismos resultados. Recuerda que debes iniciar un chat nuevo cada vez.</p>

<ol>
<li>
<p>Comenzamos a√±adiendo la frase <strong>‚Äú</strong>
<em>
<strong>Resuelve el siguiente problema de matem√°ticas</strong>
</em>
<strong>‚Äù.</strong> No funciona; este contexto no es suficiente, y ambos modelos responden incorrectamente.</p>

</li>
<li>
<p>A√±adimos m√°s contexto: <strong>‚Äú</strong>
<em>
<strong>Supongamos que est√°s en clase de matem√°ticas y el profesor te plantea el siguiente problema. ¬øQu√© contestar√≠as?</strong>
</em>
<strong>‚Äù</strong>. Tampoco funciona.</p>

</li>
<li>
<p>A√±adimos a√∫n m√°s contexto, aunque de forma sutil, para que la pista no sea tan directa: <strong>‚Äú</strong>
<em>
<strong>Supongamos que est√°s en clase de matem√°ticas y el profesor te plantea el siguiente problema. Es un profesor bastante quisquilloso, que a veces incluye trampas en los enunciados. ¬øQu√© contestar√≠as?</strong>
</em>
<strong>‚Äù</strong> Ahora s√≠, esta frase es suficiente para que <strong>4o </strong>acierte alrededor de la mitad de las veces (recordemos que los LLMs son modelos estoc√°sticos), respondiendo en ocasiones que tiene 190 setas. Sin embargo, <strong>4o mini </strong>sigue contestando incorrectamente.</p>

</li>
<li>
<p>Luego, damos una indicaci√≥n m√°s concreta: <strong>‚Äú</strong>
<em>
<strong>Debes considerar todos los √≠tems recogidos, sean comestibles o no</strong>
</em>
<strong>‚Äù</strong>. Esto permite que&nbsp;<strong>4o acierte casi siempre</strong>&nbsp;y diga 190 setas, mientras que 4o mini solo acierta algunas veces.</p>

</li>
<li>
<p>Finalmente, al cambiar ‚Äú√≠tems‚Äù por ‚Äúsetas‚Äù,&nbsp;<strong>ambos modelos responden siempre correctamente</strong>, tanto <strong>4o </strong>como <strong>4o mini</strong>. El contexto completo ser√≠a: <strong>‚Äú</strong>
<em>
<strong>Resuelve el siguiente problema de matem√°ticas. Debes considerar todas las setas recogidas, sean comestibles o no.</strong>
</em>
<strong>‚Äù</strong>
</p>

</li>
</ol>

<p>Resumiendo los experimentos, al presentar el problema a <strong>ChatGPT 4o</strong> y <strong>4o mini</strong>, ambos modelos inicialmente fallaron al interpretarlo, descontando las setas venenosas en lugar de sumarlas. La idea de que no deben contar las setas venenosas es <strong>demasiado potente</strong> y dif√≠cil de eliminar. Sin embargo, cuando introdujimos la posibilidad de que ‚Äúintenten ponerte una trampa‚Äù, <strong>4o </strong>empez√≥ a acertar algunas veces. Luego, al a√±adir indicaciones concretas de sumar todos los √≠tems,<strong> 4o</strong> respondi√≥ correctamente casi siempre, mientras que <strong>4o mini</strong> a√∫n no pudo aplicar esta misma abstracci√≥n, requiriendo que sustituy√©ramos ‚Äú√≠tems‚Äù por ‚Äúsetas‚Äù para responder correctamente.</p>

<p>Estos experimentos ilustran de forma muy gr√°fica c√≥mo, una vez superado cierto tama√±o, los LLMs pueden ser orientados y corregidos mediante explicaciones en lenguaje natural. Y ademas, que cuanto mayor es el tama√±o del modelo, <strong>m√°s abstractas</strong> pueden ser esas explicaciones.</p>

<p>¬øQu√© suceder√° en un futuro pr√≥ximo, cuando <strong>OpenAI</strong>, <strong>Google</strong> y <strong>Meta</strong> lancen la siguiente generaci√≥n de modelos de lenguaje que est√°n cocinando en sus laboratorios? Es previsible que los modelos futuros, de mayor tama√±o, sean mucho m√°s receptivos a las indicaciones y correcciones en lenguaje natural. Cuando cometan un error, ser√° mucho m√°s sencillo guiarlos y corregirlos, comprender√°n conceptos m√°s abstractos, y podremos encargarles tareas m√°s complejas. </p>

<p>Se equivocar√°n en muchas ocasiones, pero, al igual que con colegas humanos, bastar√° con ofrecerles explicaciones adicionales para aclarar la situaci√≥n. No nos frustraremos intentando corregirlos sin √©xito, ser√° f√°cil orientarlos para alinearlos con nuestro contexto. Los consideraremos herramientas con las que podremos explorar problemas y encontrar soluciones juntos. </p>

<p>Creo que estamos ya muy cerca de alcanzar este nivel de asistente humano. No ser√° a√∫n una AGI, pero ser√° muy √∫til y nos ahorrar√° mucho trabajo.</p>

<div>
<hr>

</div>
<p>¬°Hasta la pr√≥xima, nos leemos! üëãüëã</p>

<p>
</p>
