---
title: "o3 resuelve ARC-AGI"
date: 2024-12-22
draft: false
tags:
  - "newsletter"
---
<p>Cuando a principios de la semana envi√© el √∫ltimo art√≠culo, pensaba que iba a ser el √∫ltimo del a√±o. Quer√≠a terminar el a√±o hablando <a href="/posts/el-dogma-del-deep-learning/">del dogma del </a>
<em>
<a href="/posts/el-dogma-del-deep-learning/">deep learning</a>,</em> abriendo el camino para una futura continuaci√≥n en la que hablara sobre la <strong>consciencia</strong>. Y quer√≠a dejar quieta la newsletter unas semanas y trabajar poco a poco, con tranquilidad, en este nuevo art√≠culo. </p>

<p>De hecho, le he cambiado el t√≠tulo a la newsletter, <strong>ya no se llama Quince d√≠as</strong>. As√≠ me quito la presi√≥n de tener que hacer dos entregas mensuales y de contar la actualidad. Ya hay otras newsletters muy interesantes de noticias de IA. Quiero continuar con el enfoque que le he dado en los √∫ltimos n√∫meros, en los que toco con cierta profundidad alg√∫n tema, que no tiene por qu√© ser de actualidad.</p>

<p>Pero el viernes pas√≥ algo que hay que contar aqu√≠, s√≠ o s√≠. </p>

<p>OpenAI ha hecho p√∫blicos unos resultados impresionantes de <strong>su nuevo modelo razonador o3</strong>, la siguiente versi√≥n de <strong>o1</strong>. A continuaci√≥n est√° el v√≠deo con la presentaci√≥n:</p>

<div id="youtube2-SKBG1sqdyIU" class="youtube-wrap" data-attrs="{&quot;videoId&quot;:&quot;SKBG1sqdyIU&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM">
<div class="youtube-inner">
<iframe src="https://www.youtube-nocookie.com/embed/SKBG1sqdyIU?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409">
</iframe>
</div>
</div>
<p>Todos los resultados que presentan muestran un salto espectacular en los benchmarks m√°s complicados. Por ejemplo, <strong>pasan del 48,9% al 71,7% en SWE-bench Verified</strong>, un benchmark de problemas de programaci√≥n. O <strong>pasan del 3% al 25% en el ‚ÄúAI's Frontier Math‚Äù</strong>, un test que est√° compuesto de problemas de matem√°ticas del nivel de doctorado.</p>

<p>
<img src="Screenshot 2024-12-21 at 10.52.05.png" alt="">
</p>

<p>Pero lo que ha sido realmente sorprendente ha sido que <strong>han conseguido resolver la <a href="/posts/del-1-al-15-de-junio-11-de-2024/">competici√≥n ARC-AGI</a> de Fran√ßois Chollet</strong>. En el v√≠deo de presentaci√≥n de <strong>o3</strong>, mostraron la figura anterior, en la que se muestra c√≥mo <strong>o3</strong> ha conseguido acertar un 75,7% en su versi√≥n "low" y un 87% en su versi√≥n "high". </p>

<p>¬øQu√© es esto de "low" y "high"? Como vimos en el art√≠culo en el que hablamos de <a href="/posts/como-funciona-o1-15-de-2024/">c√≥mo funciona o1</a>, estos modelos razonadores pueden afinar sus resultados cuando tienen m√°s tiempo de computaci√≥n. Los modos "low" y "high" son denominaciones que han dado los investigadores de OpenAI a un funcionamiento de <strong>o3</strong> con poco tiempo de computaci√≥n y con mucho tiempo de computaci√≥n. En la gr√°fica tambi√©n se muestra que el tiempo de computaci√≥n del modo "low" de <strong>o3</strong> es significativamente mayor que el del modo "high" de <strong>o1</strong> que consigue un 32% (no dicen si la escala horizontal es lineal o logar√≠tmica, supongo que ser√° lineal, igual que la vertical).</p>

<p>En el momento en que en la retransmisi√≥n de OpenAI apareci√≥ <a href="https://x.com/GregKamradt">Greg Kamradt</a>, presidente del premio ARC, y explic√≥ todo lo anterior, <strong>me explot√≥ la cabeza</strong>. No me lo terminaba de creer. Fui corriendo a X a comprobar las reacciones, empec√© a ver los posts de gente relacionada con la competici√≥n, y, por fin, <strong>cuando vi <a href="https://x.com/fchollet/status/1870169764762710376">la reacci√≥n del propio Chollet</a> fue cuando confirm√© que era real</strong>. El equipo de OpenAI responsable de los modelos ‚Äúo‚Äù hab√≠a hecho algo hist√≥rico, resolver ARC-AGI. Se hab√≠a resuelto en tres meses, desde la presentaci√≥n de <strong>o1</strong>, un reto propuesto para identificar capacidades de razonamiento y de inteligencia humana.</p>

<p>¬øQu√© implicaciones tiene este enorme √©xito en el desarrollo de <strong>o3</strong>?</p>

<p>La implicaci√≥n m√°s importante es que se <strong>valida el enfoque</strong> de la serie de modelos razonadores "o", y se comprueba que estos modelos integran perfectamente la intuici√≥n (System 1) de los LLMs tradicionales con alg√∫n tipo de razonamiento System 2 deductivo e iterativo. OpenAI ha encontrado los ingredientes de la sopa definitiva, la que permite combinar los dos tipos de razonamiento de los que hablamos en el <a href="/posts/francois-chollet-20-de-2024/">art√≠culo sobre Chollet</a>. Esta combinaci√≥n es clave para el futuro, porque garantiza la mejora continua de los modelos. Por un lado, cuando se consiga un nuevo modelo intuitivo mejor (GPT-5) se integrar√° f√°cilmente en el nuevo modelo "o". Y cuando se mejoren las capacidades deductivas y se abarate el coste de computaci√≥n tambi√©n se podr√° conseguir mejoras sustanciales.</p>

<p>Otra implicaci√≥n fundamental es que se confirma <strong>el papel de NVIDIA</strong> y de los fabricantes de chips. Y de la energ√≠a necesaria para alimentarlos. Quien tenga m√°s MegaFLOPS ser√° el que mejores resultados obtenga. Ilya Sutskever acaba de decir que los datos son la nueva energ√≠a f√≥sil. Tambi√©n lo es la potencia de computaci√≥n.</p>

<p>Por √∫ltimo, hay que destacar la enorme suerte (o el bien hacer) de OpenAI, que ha podido terminar el a√±o con un avance espectacular y ha encontrado con los modelos "o" una forma de avanzar en su camino hacia la AGI sin tener que echar mano de su siguiente modelo GPT. Hoy mismo, <a href="https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693">en el Wall Street Journal</a>, se detallan todos los <strong>problemas que est√°n teniendo para desarrollar GPT-5</strong>. Parece que los dos o tres pre-trainings que OpenAI ha intentado han fracasado despu√©s de meses de computaci√≥n. Un modelo 10 veces m√°s grande que GPT-4 necesita tambi√©n 10 veces m√°s cantidad de datos (como m√≠nimo) y parece que est√°n teniendo problemas con eso. El debate sobre si <a href="/posts/del-1-al-15-de-noviembre-19-de-2024/">existe un muro en el </a>
<em>
<a href="/posts/del-1-al-15-de-noviembre-19-de-2024/">deep learning</a>
</em>
<strong>todav√≠a no se ha resuelto</strong>.</p>

<p>Tambi√©n hay que aclarar que, aunque el √©xito de <strong>o3</strong> ha sido espectacular, todav√≠a no hemos alcanzado la AGI. Hay muchos elementos que <strong>faltan por integrar</strong> en estos modelos, como la posibilidad de razonar con un modelo f√≠sico del mundo, el aprendizaje continuo o la creatividad.</p>

<p>Seguiremos muy atentos durante 2025 a estos temas b√°sicos de investigaci√≥n sobre el <em>deep learning</em> y los modelos de lenguaje, que marcar√°n el futuro de la tecnolog√≠a.</p>

<p>Mientras, los avances que hemos visto en este 2024 dan para much√≠simas aplicaciones que todav√≠a est√°n pendientes de desarrollar con los modelos ya disponibles. </p>

<p>Esto no para.</p>

<div>
<hr>

</div>
<p>¬°Hasta la pr√≥xima, nos leemos! üëãüëã</p>

<p>
</p>
