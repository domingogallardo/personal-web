---
title: "Del 1 al 15 de noviembre (#19 de 2024)"
date: 2024-11-19
draft: true
tags:
  - "newsletter"
---
<p>
<img src="95db515b-87eb-4649-a29b-ab5c0262ebfc_980x653.jpeg" alt="">
</p>

<h2>El muro</h2>

<p>Para mi generaci√≥n <em>
<strong>The Wall</strong>
</em> fue un doble √°lbum de <strong>Pink Floyd</strong> que nos hipnotiz√≥ a finales de los 70 y una pel√≠cula de <strong>Alan Parker</strong> que nos alucin√≥ a principios de los 80. Recuerdo verla en el cine y salir abrumado por esas im√°genes delirantes de martillos desfilando y ni√±os convertidos en aut√≥matas por un sistema educativo alienante. Era la √©poca de <strong>Reagan</strong> y <strong>Thatcher</strong> y el muro representaba el autoritarismo, la opresi√≥n y el control. Viv√≠amos asustados porque en cualquier momento alguien pod√≠a pulsar el bot√≥n nuclear. El muro representaba todo eso, y deb√≠a ser derribado.</p>

<p>Esta √∫ltima quincena se ha hablado mucho de un muro distinto, el <strong>muro en el escalado</strong> de los modelos de lenguaje. En el&nbsp;<a href="https://youtu.be/J3SDZjYH4xY?si=bwaesm1zj66Hyd0t">episodio de esta semana</a>&nbsp;de monos estoc√°sticos y en&nbsp;<a href="https://www.error500.net/p/la-hipotesis-del-escalado-de-la-inteligencia">el art√≠culo</a>&nbsp;de Antonio Ortiz de Error500 se explica muy bien de qu√© se trata. Miradlo ah√≠ para una informaci√≥n detallada. </p>

<div id="youtube2-J3SDZjYH4xY" class="youtube-wrap" data-attrs="{&quot;videoId&quot;:&quot;J3SDZjYH4xY&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM">
<div class="youtube-inner">
<iframe src="https://www.youtube-nocookie.com/embed/J3SDZjYH4xY?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409">
</iframe>
</div>
</div>
<div class="embedded-post-wrap" data-attrs="{&quot;id&quot;:151613544,&quot;url&quot;:&quot;https://www.error500.net/p/la-hipotesis-del-escalado-de-la-inteligencia&quot;,&quot;publication_id&quot;:903887,&quot;publication_name&quot;:&quot;Error500&quot;,&quot;publication_logo_url&quot;:&quot;6c945ec4-e8e4-4598-9b36-4bde6615864a_1024x1024.png&quot;,&quot;title&quot;:&quot;La hip√≥tesis del escalado de la inteligencia artificial hasta llegar a la AGI&quot;,&quot;truncated_body_text&quot;:&quot;Es el concepto que m√°s dinero ha movilizado en el mundo los √∫ltimos dos a√±os.&quot;,&quot;date&quot;:&quot;2024-11-17T08:33:23.620Z&quot;,&quot;like_count&quot;:20,&quot;comment_count&quot;:3,&quot;bylines&quot;:[{&quot;id&quot;:1249415,&quot;name&quot;:&quot;Antonio Ortiz&quot;,&quot;handle&quot;:&quot;antonioortiz&quot;,&quot;previous_name&quot;:null,&quot;photo_url&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/5fc5a3b2-740b-41c0-acbe-18165c4d4354_2048x1536.jpeg&quot;,&quot;bio&quot;:&quot;No sabemos gran cosa, vamos saliendo del paso&quot;,&quot;profile_set_up_at&quot;:&quot;2022-03-10T07:39:00.486Z&quot;,&quot;publicationUsers&quot;:[{&quot;id&quot;:844667,&quot;user_id&quot;:1249415,&quot;publication_id&quot;:902491,&quot;role&quot;:&quot;admin&quot;,&quot;public&quot;:true,&quot;is_primary&quot;:false,&quot;publication&quot;:{&quot;id&quot;:902491,&quot;name&quot;:&quot;Causas y Azares&quot;,&quot;subdomain&quot;:&quot;causasyazares&quot;,&quot;custom_domain&quot;:null,&quot;custom_domain_optional&quot;:false,&quot;hero_text&quot;:&quot;Sobre las causas y los azares. Un dominical con las mejores lecturas de la semana, los temas importantes que no reciben tanta atenci√≥n y alguna obsesi√≥n &quot;,&quot;logo_url&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/a85270b7-9f17-4186-ac00-365ad9a35c7d_512x512.png&quot;,&quot;author_id&quot;:1249415,&quot;theme_var_background_pop&quot;:&quot;#8AE1A2&quot;,&quot;created_at&quot;:&quot;2022-05-24T14:36:26.490Z&quot;,&quot;rss_website_url&quot;:null,&quot;email_from_name&quot;:&quot;Causas y Azares por Antonio Ortiz&quot;,&quot;copyright&quot;:&quot;Antonio Ortiz&quot;,&quot;founding_plan_name&quot;:null,&quot;community_enabled&quot;:true,&quot;invite_only&quot;:false,&quot;payments_state&quot;:&quot;disabled&quot;,&quot;language&quot;:&quot;es&quot;,&quot;explicit&quot;:false,&quot;is_personal_mode&quot;:false}},{&quot;id&quot;:846125,&quot;user_id&quot;:1249415,&quot;publication_id&quot;:903887,&quot;role&quot;:&quot;admin&quot;,&quot;public&quot;:true,&quot;is_primary&quot;:false,&quot;publication&quot;:{&quot;id&quot;:903887,&quot;name&quot;:&quot;Error500&quot;,&quot;subdomain&quot;:&quot;error500&quot;,&quot;custom_domain&quot;:&quot;www.error500.net&quot;,&quot;custom_domain_optional&quot;:false,&quot;hero_text&quot;:&quot;An√°lisis tecnol√≥gico: tendencias, noticias y comentarios sobre el mundo digital, la innovaci√≥n y la inteligencia artificial&quot;,&quot;logo_url&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/6c945ec4-e8e4-4598-9b36-4bde6615864a_1024x1024.png&quot;,&quot;author_id&quot;:1249415,&quot;theme_var_background_pop&quot;:&quot;#2096FF&quot;,&quot;created_at&quot;:&quot;2022-05-25T09:28:32.182Z&quot;,&quot;rss_website_url&quot;:null,&quot;email_from_name&quot;:&quot;Antonio Ortiz de Error 500&quot;,&quot;copyright&quot;:&quot;Antonio Ortiz&quot;,&quot;founding_plan_name&quot;:null,&quot;community_enabled&quot;:true,&quot;invite_only&quot;:false,&quot;payments_state&quot;:&quot;disabled&quot;,&quot;language&quot;:&quot;es&quot;,&quot;explicit&quot;:false,&quot;is_personal_mode&quot;:false}},{&quot;id&quot;:1277545,&quot;user_id&quot;:1249415,&quot;publication_id&quot;:1311767,&quot;role&quot;:&quot;admin&quot;,&quot;public&quot;:true,&quot;is_primary&quot;:false,&quot;publication&quot;:{&quot;id&quot;:1311767,&quot;name&quot;:&quot;monos estoc√°sticos&quot;,&quot;subdomain&quot;:&quot;monosestocasticos&quot;,&quot;custom_domain&quot;:&quot;www.monosestocasticos.com&quot;,&quot;custom_domain_optional&quot;:false,&quot;hero_text&quot;:&quot;La newsletter sobre inteligencia artificial en espa√±ol\\n\\nTodas las noticias sobre IA, los debates m√°s interesantes y la evoluci√≥n de ChatGPT, Stable Diffusion y muchos servicios m√°s&quot;,&quot;logo_url&quot;:&quot;123e0817-d3f7-4b86-b82f-63cbb3a5e22c_1024x1024.png&quot;,&quot;author_id&quot;-106274467,&quot;theme_var_background_pop&quot;-&quot; y Antonio&quot;,&quot;copyright&quot;:&quot;Halora Medios&quot;,&quot;founding_plan_name&quot;:null,&quot;community_enabled&quot;:true,&quot;invite_only&quot;:false,&quot;payments_state&quot;:&quot;disabled&quot;,&quot;language&quot;:&quot;es&quot;,&quot;explicit&quot;:false,&quot;is_personal_mode&quot;:false}}],&quot;is_guest&quot;:false,&quot;bestseller_tier&quot;:null}],&quot;utm_campaign&quot;:null,&quot;belowTheFold&quot;:false,&quot;type&quot;:&quot;newsletter&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="EmbeddedPostToDOM">
<a class="embedded-post" native="true" href="https://www.error500.net/p/la-hipotesis-del-escalado-de-la-inteligencia?utm_source=substack&amp;utm_campaign=post_embed&amp;utm_medium=web">
<div class="embedded-post-header">
<img class="embedded-post-publication-logo" src="6c945ec4-e8e4-4598-9b36-4bde6615864a_1024x1024.png">
<span class="embedded-post-publication-name">Error500</span>
</div>
<div class="embedded-post-title-wrapper">
<div class="embedded-post-title">La hip√≥tesis del escalado de la inteligencia artificial hasta llegar a la AGI</div>
</div>
<div class="embedded-post-body">Es el concepto que m√°s dinero ha movilizado en el mundo los √∫ltimos dos a√±os‚Ä¶</div>
<div class="embedded-post-cta-wrapper">
<span class="embedded-post-cta">Read more</span>
</div>
<div class="embedded-post-meta">a year ago ¬∑ 20 likes ¬∑ 3 comments ¬∑ Antonio Ortiz</div>
</a>
</div>
<p>Hoy solo voy a dar unas pinceladas r√°pidas, con algunos enlaces y con mi opini√≥n personal.</p>

<p>El <strong>9 de noviembre</strong>, <strong>The Information</strong> public√≥ el art√≠culo <em>
<strong>OpenAI Shifts Strategy as Rate of ‚ÄòGPT‚Äô AI Improvements Slows</strong>
</em>. No he podido leer el art√≠culo porque es de pago, y no he encontrado ninguna versi√≥n libre. Unos d√≠as despu√©s <strong>Reuters</strong> public√≥&nbsp;<a href="https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/">otro art√≠culo</a>&nbsp;que inclu√≠a algunas declaraciones de <strong>Ilya Sutskever</strong> en la l√≠nea de que hay que probar cosas nuevas, y no es suficiente con solo escalar. Y entre medias se conoci√≥&nbsp;<a href="https://arxiv.org/abs/2411.04330">un paper</a>, <em>
<strong>Scaling Laws for Precision</strong>
</em>,&nbsp;<a href="https://x.com/tanishqkumar07/status/1856045600355352753">hilo en X</a>&nbsp;que tambi√©n evidencia problemas en el escalado de los modelos. Todo negativo, nada positivo. </p>

<p>A todo esto hay que sumar que llevamos un a√±o y medio desde que se lanz√≥ <strong>GPT-4</strong> y todav√≠a no se ha lanzado ning√∫n modelo m√°s grande. No ha aparecido ni <strong>GPT-5</strong>, ni <strong>Claude 3</strong>, ni <strong>Gemini 2</strong>. El siguiente paso en el escalado (un modelo con m√°s de 10T par√°metros) est√° tardando en llegar. </p>

<p>Todo esto ha empezado a arrojar sospechas sobre la gran hip√≥tesis que est√° moviendo la industria en los √∫ltimos a√±os. ¬øVan a ser in√∫tiles todos los grandes planes de las tecnol√≥gicas de construir enormes centros de datos en los pr√≥ximos a√±os? ¬øVa a pinchar NVIDIA? ¬øVa a explotar la burbuja?</p>

<p>Menos mal que todo se calm√≥ al final de la quincena, cuando <strong>Altman</strong>&nbsp;<a href="https://x.com/sama/status/1856941766915641580">nos dio una alegr√≠a</a>, diciendo que todo esto no son m√°s que inventos, que no hay muro.</p>

<p>
<img src="Captura de pantalla 2024-11-15 a las 14.08.58.png" alt="">
</p>

<p>¬øPodemos creer lo que dice <strong>Altman</strong>? Vamos con mi opini√≥n personal. Esta quincena he escuchando dos entrevistas muy interesantes. La primera ha sido la de&nbsp;<a href="https://youtu.be/a42key59cZQ?si=yureQV6AgtYYNlg1">Dwarkesh Patel a Gwern Branwen</a>
<a class="footnote-anchor" data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1" href="#footnote-1" target="_self">1</a>, una de las primeras personas en proponer la&nbsp;<a href="https://gwern.net/scaling-hypothesis">hip√≥tesis de escalado</a>. </p>

<div id="youtube2-a42key59cZQ" class="youtube-wrap" data-attrs="{&quot;videoId&quot;:&quot;a42key59cZQ&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM">
<div class="youtube-inner">
<iframe src="https://www.youtube-nocookie.com/embed/a42key59cZQ?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409">
</iframe>
</div>
</div>
<p>Aunque la hip√≥tesis ya se hab√≠a planteado en un <a href="https://arxiv.org/abs/2001.08361">paper</a>&nbsp;de <strong>OpenAI</strong> de enero de 2020, <em>
<strong>Scaling Laws for Neural Language Models</strong>
</em> (y mucho antes, en 2015, <strong>Andrej Karpathy</strong> se hab√≠a adelantado al futuro con su post <em>
<strong>
<a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>
</strong>
</em>), fue el post de <strong>Gwern</strong> el que se hizo viral y el que dio a conocer esta idea al gran p√∫blico. </p>

<p>La otra entrevista de la quincena ha sido en el podcast <strong>Machine Learning Street Talk</strong>
<a href="https://youtu.be/JTU8Ha4Jyfc?si=gC42K37DakcQLpl1">con Fran√ßois Chollet</a>. </p>

<div id="youtube2-JTU8Ha4Jyfc" class="youtube-wrap" data-attrs="{&quot;videoId&quot;:&quot;JTU8Ha4Jyfc&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM">
<div class="youtube-inner">
<iframe src="https://www.youtube-nocookie.com/embed/JTU8Ha4Jyfc?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409">
</iframe>
</div>
</div>
<p>Es una entrevista muy t√©cnica, con detalles muy interesantes. Estoy estudi√°ndola en profundidad y har√© un comentario en un pr√≥ximo art√≠culo.</p>

<p>En las dos entrevistas se habla de c√≥mo se puede explicar el funcionamiento de los LLMs. Lo que hacen estas redes neuronales es aprender un ingente n√∫mero de programas (funciones) que predicen el siguiente token y construir a su vez nuevas funciones, explorando el enorme espacio de posibles combinaciones y qued√°ndose con las mejores.</p>

<p>Aunque <strong>Chollet</strong> ha dicho muchas veces que los LLMs no pueden llegar a ser AGIs (y, de ah√≠, su&nbsp;<a href="https://arcprize.org/">competici√≥n de ARC</a>), su cr√≠tica se basa en su <strong>incapacidad de enfrentarse a las novedades</strong> y la poca eficiencia del algoritmo de descenso por gradiente para recombinar la estructura del LLM a partir de unas pocas muestras. A diferencia de lo que muchos han entendido, <strong>Chollet</strong> no dice que los LLMs no son capaces de generalizar. De hecho dice expl√≠citamente en la entrevista que los LLMs s√≠ que construyen modelos a partir de los datos de entrenamiento. Y que esos modelos son funciones que definen curvas que permiten a los LLMs interpolar. Pero (y esta es mi interpretaci√≥n) esas curvas pueden serlo en un espacio s√∫per abstracto, como por ejemplo, el estilo literario o el an√°lisis de opinion (positiva, negativa o neutra) de un texto.</p>

<p>
<strong>Gwern</strong> habla tambi√©n de lo mismo, aunque usa el t√©rmino M√°quinas de Turing (MT) para referirse a lo que aprenden los LLMs. Es lo mismo. Cuando hablamos de MTs estamos hablando de algoritmos. Los LLMs <strong>aprenden algoritmos </strong>que permiten predecir el siguiente token de una secuencia. Como dec√≠a <strong>Karpathy</strong> las redes neuronales son irrazonablemente efectivas en ello o como dec√≠a <strong>Sutskever</strong>
<a href="https://x.com/tsarnick/status/1765997009763488066">los modelos solo quieren aprender</a>.</p>

<p>Entonces, la versi√≥n de la tesis del escalado que ahora mismo tengo en la cabeza se podr√≠a formular de la siguientes forma:</p>

<ol>
<li>
<p>Los LLMs crean una cantidad ingente de <strong>funciones</strong> que les sirven para predecir el siguiente token.</p>

</li>
<li>
<p>Cuanto m√°s grandes son los LLMs (y se han entrenado como un n√∫mero mayor de datos, y han estado m√°s tiempo siendo entrenados) estas funciones tienen un <strong>nivel de abstracci√≥n mayor</strong> y pueden generalizar mejor los datos de entrenamiento. Por ejemplo, los LLMs m√°s peque√±os son capaces de detectar regularidades sint√°cticas (despu√©s de un art√≠culo viene un nombre) y los m√°s grandes detectan regularidades sem√°nticas (‚Äúel mar es azul‚Äù, ‚Äúuna mesa puede tener objetos encima‚Äù o ‚Äúun coche circula por una carretera‚Äù).</p>

</li>
<li>
<p>S√≠ que me creo lo que han dicho gente de la industria, que todav√≠a se pueden escalar dos generaciones m√°s los modelos actuales. S√© que todas estas personas tienen intereses comerciales, pero <strong>no veo razones que impidan este escalado</strong>. No creo, por ejemplo, que haya un muro en los datos de entrenamiento. Se pueden generar de forma artificial o por expertos que escriben&nbsp;<a href="https://x.com/karpathy/status/1857584163140030710">libros de ejercicios</a>. Tambi√©n queda por explorar el uso de secuencias reales de v√≠deo de 25 fps, no los 1 fps que se est√°n usando ahora. Pero para eso har√° falta mucha m√°s potencia computacional.</p>

</li>
</ol>

<p>Veremos. Como dice <strong>Antonio Ortiz</strong> en su art√≠culo, lo bueno es que no tardaremos mucho en comprobar si esto es as√≠ o no. El a√±o que viene debe ser el a√±o en el que aparezca el pr√≥ximo gran modelo, ya sea <strong>GPT-5</strong>, <strong>Gemini 2</strong> o <strong>Grok 3</strong>. Pronto sabremos si el escalado sigue funcionando.</p>

<p>
<img src="I want to believe.jpg" alt="">
</p>

<p>Como <strong>Mulder</strong>, yo quiero creer. Lleg√≥ <strong>Gorvachov</strong>. <strong>Reagan</strong> gan√≥ la Guerra Fr√≠a y otro muro cay√≥ en 1986. Pero ahora, 40 a√±os despu√©s, estamos m√°s o menos igual que en los 80, incluso peor.</p>

<p>Todos los de mi generaci√≥n vimos tambi√©n en esa √©poca la pel√≠cula en la que el superordenador <strong>WOPR</strong> estuvo a punto de desencadenar la guerra nuclear definitiva<a class="footnote-anchor" data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2" href="#footnote-2" target="_self">2</a>. El ordenador ten√≠a una puerta trasera con la que se pod√≠a acceder a su verdadera personalidad. Se llamaba realmente <strong>Joshua</strong> y, al final, consigue generalizar correctamente y alinearse con los valores humanos:</p>

<blockquote>
<p>Este es un juego extra√±o. La √∫nica forma de ganarlo es no empezarlo.</p>

</blockquote>

<p>
<strong>Stephen Falken</strong> hab√≠a programado ese ordenador y le hab√≠a llamado <strong>Joshua</strong> en honor a su hijo fallecido. Las razones que tienen los Falken de hoy en d√≠a son m√°s prosaicas. Pero me gustar√≠a creer que el resultado va a ser el mismo. Que <strong>Altman</strong>, <strong>Amodei</strong>, <strong>Sutskever</strong>, <strong>Karpathy</strong>, <strong>Chollet</strong>, <strong>Murati</strong> y dem√°s residentes en <strong>San Francisco</strong> nos van a llevar a la tecno-utop√≠a de&nbsp;<a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">GPT-2030</a>, llena de <a href="https://darioamodei.com/machines-of-loving-grace">m√°quinas de gracia compasiva</a>.</p>

<div>
<hr>

</div>
<p>¬°Hasta la pr√≥xima, nos leemos! üëãüëã</p>

<p>
</p>

<div class="footnote" data-component-name="FootnoteToDOM">
<a id="footnote-1" href="#footnote-anchor-1" class="footnote-number" contenteditable="false" target="_self">1</a>
<div class="footnote-content">
<p>
<strong>Gwern Branwen</strong> es un apodo. Es una figura an√≥nima que lleva a√±os construyendo <a href="https://www.gwern.net/">Gwern.net</a>,&nbsp;un ingente hipertexto en el que va anotando todas sus ideas. No solo escribe el contenido, sino que es el autor del software que lo gestiona,&nbsp;<a href="https://github.com/gwern/gwern.net">disponible en abierto en GitHub</a>. La entrevista es excepcional. No solo por su contenido, sino por su valor como la primera aparici√≥n p√∫blica de un personaje brillante y enigm√°tico. Aunque es una aparici√≥n p√∫blica parcial, porque la imagen de v√≠deo est√° generada por ordenador y la voz no es la del propio <strong>Gwern</strong>. <strong>Gwern</strong> declara en la entrevista que es sordo desde su infancia, y que tiene reparos a aparecer con su propia voz. <br>
<br>Parece que la entrevista va a ser un punto de inflexi√≥n en su vida y que va a dejar de vivir en una modesta casa con 12.000 d√≥lares al a√±o para pasar a&nbsp;<a href="https://x.com/Suhail/status/1857102763249004655">mudarse a San Francisco</a>.</p>

</div>
</div>
<div class="footnote" data-component-name="FootnoteToDOM">
<a id="footnote-2" href="#footnote-anchor-2" class="footnote-number" contenteditable="false" target="_self">2</a>
<div class="footnote-content">
<p>Algunos quisimos ser<strong> Mathew Brodderick</strong>, nos compramos un <strong>Spectrum</strong> y nos enganchamos para siempre a esto de la inform√°tica y la programaci√≥n.</p>

<p>
</p>

</div>
</div>
