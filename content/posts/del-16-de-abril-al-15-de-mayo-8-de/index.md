---
title: "Del 16 de abril al 15 de mayo (#8 de 2024)"
date: 2024-05-17
draft: false
tags:
  - "newsletter"
---
<p>üëãüëã ¬°Hola, soy Domingo!</p>

<p>Despu√©s de una quincena de retraso y una vez resueltos (m√°s o menos) los problemas de organizaci√≥n, gesti√≥n del tiempo y procrastinaci√≥n, aqu√≠ estoy con un nuevo n√∫mero. Hoy toca repasar no una, sino dos quincenas.</p>

<p>Un anuncio: a partir de ahora, tengo la intenci√≥n de publicar siempre los viernes, despu√©s de la primera o la segunda quincena del mes. La mayor√≠a de las newsletters que leo se publican en un d√≠a fijo de la semana, y me parece una buena idea probarlo. As√≠, ya sabr√©is que un viernes s√≠ y otro no llegar√© a vuestro correo electr√≥nico. Y algunos viernes en los que no toque, habr√° alguna sorpresa. Ya ver√©is la semana que viene.</p>

<p>Muchas noticias en un mes. Sobre todo muchos nuevos modelos. ¬°Vamos all√°! ¬°Muchas gracias por leerme!</p>

<h2>üóû Noticias</h2>

<p>
</p>

<p>1Ô∏è‚É£ El<strong> 18 de abril</strong>, <strong>Meta</strong> lanz√≥ sus<strong> nuevas versiones de Llama</strong>, los modelos Meta Llama 3 [<a href="https://llama.meta.com/llama3/">Build the future of AI with Meta Llama 3 - meta.com</a> y <a href="https://ai.meta.com/blog/meta-llama-3/">Introducing Meta Llama 3: The most capable openly available LLM to date - meta.com</a>]. </p>

<p>Recordemos que los modelos Llama son LLMs abiertos, disponibles para descargarlos, refinarlos y usarlos en cualquier aplicaci√≥n. Eso s√≠, si usamos estos modelos o creamos modelos nuevos basados en ellos, tendremos que incluir la frase ‚ÄúBuilt with Meta Llama 3‚Äù o incluir el nombre ‚ÄúLlama 3‚Äù al principio del nombre del modelo [<a href="https://llama.meta.com/llama3/license/#">META LLAMA 3 COMMUNITY LICENSE AGREEMENT - meta.com</a>].</p>

<p>En concreto, Meta ha publicado dos modelos, uno de 8 mil millones y otro de 70 mil millones de par√°metros: Llama 3 8B y Llama 3 70B. Seg√∫n los benchmarks publicados por la propia Meta, estos modelos son los mejores en comparaci√≥n con los de tama√±os similares.</p>

<p>
<img src="86510236-9946-4ad1-939b-9d645352f5d6_3840x2160.png" alt="">
</p>

<p>Todav√≠a est√°n entrenando el modelo m√°s grande, de m√°s de 400 mil millones de par√°metros (llamado Meta Llama 3 400B+). Sin embargo, adelantan unos resultados iniciales que lo sit√∫an por delante de GPT-4 y a la par de Claude 3 Opus.</p>

<p>
<img src="2a947743-c63f-4482-8c58-4ee6dceedc35_1632x1420.png" alt="">
</p>

<p>Rendimiento de los √∫ltimos LLMs presentados en los benchmarks m√°s importantes [<a href="https://openai.com/index/hello-gpt-4o/">Hello GPT-4o - openai.com</a>]</p>

<p>Es muy interesante que estos modelos han sido construidos con procesos muy cuidadosos (por ejemplo, utilizando datasets y textos de alta calidad) que les han permitido obtener mejores resultados que modelos previos mucho m√°s grandes. Por ejemplo, el modelo m√°s peque√±o, Llama 3 8B, alcanza puntuaciones bastante mejores que el modelo de 70B de hace un a√±o (Llama 2 70B). De forma similar, Llama 3 70B obtiene mejores puntuaciones que GPT-3.5, con 175B par√°metros.</p>

<p>
<img src="f39565ad-44bf-4d81-aee2-b620a64d67a0_1650x588.png" alt="">
</p>

<p>Puntuaci√≥n en los benchmarks m√°s importantes de los modelos de Meta [<a href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md">Llama 3 Model Card - github.com</a>].</p>

<p>El tama√±o del modelo no solo es importante para su rendimiento, sino que tambi√©n determina el espacio que ocupa. El modelo Llama 3 7B tiene un tama√±o de poco m√°s de 13 GB. En su versi√≥n cuantizada, que reduce la precisi√≥n de los par√°metros para ahorrar espacio, ocupa aproximadamente 4,21 GB. Este tama√±o permitir√≠a ejecutarlo en un dispositivo m√≥vil de altas prestaciones. Aunque existen t√©cnicas avanzadas que permiten usar los modelos desde la memoria de almacenamiento SSD [<a href="https://www.macrumors.com/2023/12/21/apple-ai-researchers-run-llms-iphones/">Apple Develops Breakthrough Method for Running LLMs on iPhones - macrumors.com</a>], lo m√°s habitual es que todos los pesos del modelo deban estar cargados en la memoria del dispositivo para poder ejecutarlos. Por ejemplo, mi iPhone 12 tiene 4 GB de memoria RAM y no podr√≠a ejecutarlo. Los √∫ltimos modelos (iPhone 15) tienen 6 GB en sus configuraciones b√°sicas (iPhone 15 y 15 Plus) y 8 GB en sus configuraciones avanzadas (15 Pro y 15 Pro Max). Los modelos b√°sicos se quedar√≠an muy escasos, mientras que los Pro s√≠ que podr√≠an ejecutarlo. La posibilidad de uso de un LLM local seguro que se va a convertir en un factor diferenciador de los m√≥viles a partir de ahora.</p>

<p>Si quer√©is trastear los modelos, pod√©is encontrarlos en Hugging Face [<a href="https://huggingface.co/meta-llama">Meta Llama - hagglingface.co</a>] y leer el post que han publicado en el propio Hugging Face [<a href="https://huggingface.co/blog/llama3">Welcome Llama 3 - Meta‚Äôs new open LLM - huggingface.co</a>].</p>

<p>2Ô∏è‚É£ El <strong>23 de abril</strong>,<strong> Microsoft</strong> lanz√≥ sus <strong>modelos Phi-3</strong> [<a href="https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/">Introducing Phi-3: Redefining what‚Äôs possible with SLMs - microsoft.com</a> y <a href="https://arstechnica.com/information-technology/2024/04/microsofts-phi-3-shows-the-surprising-power-of-small-locally-run-ai-language-models/">Microsoft‚Äôs Phi-3 shows the surprising power of small, locally run AI language models - arstechnica.com</a>]. Se trata de modelos peque√±os:</p>

<ul>
<li>
<p>Phi-3-mini, con 3.8 mil millones de par√°metros y una ventana de contexto de 4 mil tokens (aunque Microsoft tambi√©n ha introducido una versi√≥n de 128K tokens llamada ‚Äúphi-3-mini-128K‚Äù).</p>

</li>
<li>
<p>Phi-3-small, con 7 mil millones (7B) de par√°metros.</p>

</li>
<li>
<p>Phi-3-medium, con 14 mil millones (14B) de par√°metros.</p>

</li>
</ul>

<p>Los modelos se describen con detalle en el art√≠culo publicado en arXiv [<a href="https://arxiv.org/abs/2404.14219">Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone</a>], en el que demuestran que el modelo m√°s peque√±o (cuantizado a 4 bits) se puede ejecutar en un iPhone 14 Pro (curioso que Microsoft pruebe sus modelos de lenguaje en dispositivos de Apple).</p>

<p>En el art√≠culo se presenta la evaluaci√≥n de estos modelos, demostr√°ndose que obtienen en los benchmarks resultados similares o incluso algo mejores que otros modelos peque√±os y comparables en algunos casos a GPT-3.5.</p>

<p>
<img src="8771d844-299d-447d-9ef1-2fdd64c0e714_1876x1046.png" alt="">
</p>

<p>Resultados de los modelos peque√±os de Microsoft en benchmarks seleccionados, comparados con otros modelos peque√±os y GPT-3.5.</p>

<p>Los modelos tambi√©n son abiertos y est√°n disponibles en Hugging Face [<a href="https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3">Phi-3 family of models - huggingface.co</a>].</p>

<p>Se avecina una competici√≥n importante en los modelos peque√±os, en los que Apple ya est√° haciendo sus primeros pinitos (ver la siguiente noticia).</p>

<p>3Ô∏è‚É£ El <strong>22 de abril</strong>, <strong>Apple</strong> public√≥ varios <strong>modelos peque√±os</strong> llamados OpenELM [<a href="https://arstechnica.com/information-technology/2024/04/apple-releases-eight-small-ai-language-models-aimed-at-on-device-use/">Apple releases eight small AI language models aimed at on-device use - arstechnica.com</a> y <a href="https://arxiv.org/abs/2404.14619v1">OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework - arxiv.org</a>]. En el n√∫mero anterior, mencionamos que el departamento de investigaci√≥n de Apple estaba publicando resultados sobre sus primeros modelos de lenguaje. Pues bien, algunos modelos ya est√°n disponibles en abierto en Hugging Face [<a href="https://huggingface.co/apple/OpenELM-3B-Instruct">OpenELM - huggingface.co</a>] y pueden ser probados por la comunidad. </p>

<p>Se trata de modelos muy b√°sicos, con resultados bastante bajos comparados con modelos del mismo tama√±o. Aunque Apple se est√° poniendo las pilas en el √∫ltimo a√±o, todav√≠a est√° muy lejos de grupos de investigaci√≥n como los de Meta y Microsoft (que, a su vez, est√°n lejos de los de Google, Anthropic y OpenAI).</p>

<p>He hecho una peque√±a investigaci√≥n, usando Wayback Machine, de la evoluci√≥n del n√∫mero de investigadores en la organizaci√≥n de Apple de Hugging Face con el resultado que se muestra en la siguiente gr√°fica:</p>

<p>
<img src="cc4b5853-c39b-497b-9f4a-287ae66927c1_1564x846.png" alt="">
</p>

<p>Evoluci√≥n del n√∫mero de investigadores en la organizaci√≥n de Apple en Hugging Face [<a href="https://web.archive.org/web/20030315000000*/https://huggingface.co/apple">Wayback Machine - archive.org</a>].</p>

<p>El n√∫mero de investigadores de Apple en Hugging Face ha pasado de solo 4 hace dos a√±os a 308 en la fecha en que estoy escribiendo esto. La gr√°fica la hice hace una semana, cuando hab√≠a 284 investigadores. ¬°En una semana han a√±adido a 24 investigadores m√°s!</p>

<p>Ahora la duda est√° en qu√© modelos va a usar Apple en sus nuevos iPhone 16, que se lanzar√°n el pr√≥ximo septiembre y que, esta vez s√≠, seguro que vendr√°n cargados de novedades relacionadas con la IA. Seg√∫n Mark Gurman, la estrategia de Apple es usar modelos propios para las APIs, tanto en el dispositivo como en la nube, y presentar un asistente (chat bot) resultado de un acuerdo con OpenAI.</p>

<p>
<img src="16d7754f-b38c-451b-a29b-4c945e28a19c_1190x400.png" alt="">
</p>

<p>
<a href="https://x.com/markgurman/status/1789460505150792030">Post de Mark Gurman en X</a> sobre los modelos de lenguaje que va a anunciar Apple en la pr√≥xima WWDC.</p>

<p>Lo sabremos pronto, en la conferencia de desarrolladores de Apple que se celebrar√° en menos de un mes [<a href="https://developer.apple.com/wwdc24/">WWDC24 - apple.com</a>] en la que se presentar√° iOS 18 y todas las novedades de IA que incluir√°.</p>

<p>4Ô∏è‚É£ Y llegamos al punto √°lgido del mes: el <strong>13 de mayo</strong>,<strong> OpenAI</strong> realiz√≥ un evento especial en el que presentaron su nuevo modelo: <strong>GPT-4o</strong> [<a href="https://www.youtube.com/watch?v=DQacCB9tDaw">Introducing GPT-4o - youtube.com</a>]. Despu√©s del 4 va la letra ‚Äúo‚Äù, de ‚Äúomni‚Äù (total), no es el n√∫mero cero. </p>

<p>Se trata de un modelo multimodal desde el principio. Es un proyecto que empez√≥ hace m√°s de un a√±o, como comenta Prafulla Dhariwal, su director.</p>

<p>
<img src="b901e2a3-d839-4ecd-899e-e45099c59eed_1180x284.png" alt="">
</p>

<p>
<a href="https://x.com/prafdhar/status/1790790264178774351">Post en X</a> del director del proyecto GPT-4o en OpenAI.</p>

<p>El hecho de que el modelo sea multimodal significa que ha sido entrenado desde cero con texto, audio e im√°genes. Aunque trabaja con v√≠deo no ha sido entrenado con secuencias de v√≠deos<a class="footnote-anchor" data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1" href="#footnote-1" target="_self">1</a>, sino que descompone el v√≠deo que est√° viendo en instant√°neas y analiza cada una de ellas. Tambi√©n es capaz de producir los mismos elementos con los que ha sido entrenado: texto, audio o v√≠deo.</p>

<p>La versi√≥n que han puesto en producci√≥n es la que produce texto. En la demo que hicieron en el evento, mostraron c√≥mo el modelo era capaz de generar audio. Y est√°n terminando de probar la generaci√≥n de im√°genes (por el propio modelo, no por modelos externos como DALL-E) antes de ponerla en producci√≥n.</p>

<p>En el evento de OpenAI se mostr√≥ el funcionamiento del modelo en forma de asistente. Es capaz de entender lo que le decimos, el tono en el que se lo decimos, lo que le ense√±amos, y es capaz de responder con voz. Una voz s√∫per natural, que expresa emociones y que nos felicita y anima. Evidentemente, ha sido entrenado para que se parezca a la Scarlett Johansson en ‚ÄúHer‚Äù. </p>

<p>Mirad qu√© impresionante:</p>

<div class="native-video-embed" data-component-name="VideoPlaceholder" data-attrs="{&quot;mediaUploadId&quot;:&quot;b66aaef5-0a08-44a9-9c5b-666c646a7c4a&quot;,&quot;duration&quot;:null}">
</div>
<p>Tal y como hace notar Antonio Ortiz [<a href="https://cuonda.com/monos-estocasticos/el-lenguaje-de-programacion-del-futuro-es-seducir-a-scarlett-johansson">Monos estoc√°sticos 2x17 - cuonda.com</a>], hay que fijarse en el momento en el que la IA se equivoca al principio y dice que ‚Äúve la imagen‚Äù antes de que se la ense√±en. Lo apabullante viene despu√©s, cuando pide disculpas y lo hace con una entonaci√≥n espec√≠fica, totalmente humana, que transmite algo de verg√ºenza por el error. </p>

<p>Si vemos el v√≠deo con cuidado, podemos notar que la IA produce varias entonaciones muy distintas a lo largo de la conversaci√≥n. Es incre√≠ble que esto sea el resultado de un modelo entrenado solo para generar el siguiente token m√°s probable. Ya hemos visto que esto funciona con el texto. Ahora, OpenAI ha demostrado que la idea tambi√©n funciona cuando el siguiente token puede ser tanto un trozo de palabra como un fragmento de imagen o de audio. Y tambi√©n es muy importante el ‚Äúfine-tuning‚Äù posterior, en el que se adapta la respuesta producida por el modelo a las preferencias que interesan, en lo que se denomina RLHF (Reinforcement Learning with Human Feedback).</p>

<p>Dejadme recalcar una cosa muy importante, porque se ha producido mucha confusi√≥n al respecto. En la aplicaci√≥n actual de ChatGPT es posible realizar una conversaci√≥n. Pero no es lo mismo en absoluto que lo que hace GPT-4o. Lo que tenemos ahora en nuestros dispositivos son tres modelos distintos unidos por una aplicaci√≥n: un modelo que reconoce el habla y la transcribe a texto, otro modelo (GPT-4) que responde con una entrada de texto y devuelve otro texto, y otro modelo que transforma el texto de respuesta en habla. En GPT-4o es totalmente diferente. Hay solo un √∫nico modelo, una √∫nica red neuronal que recibe tokens en cualquiera de las modalidades y devuelve como respuesta otros tokens (que pueden ser texto, audio o incluso una imagen). La propia red neuronal, el propio modelo, es el que produce las entonaciones (y tambi√©n las entiende). No hay un postproceso ni un algoritmo espec√≠fico que convierta el texto en audio. Es alucinante.</p>

<p>Otra de las caracter√≠sticas m√°s importantes del nuevo asistente presentado es su peque√±√≠sima latencia (responde de forma casi instant√°nea) y la posibilidad de ser interrumpido en cualquier momento, habl√°ndole. La IA nos est√° escuchando continuamente y deja de hablar en ese momento para escucharnos y volver a contestar.</p>

<p>En cuanto a rendimiento en generaci√≥n de texto, el modelo resultante es mejor que la √∫ltima versi√≥n de GPT-4 y se ha colocado directamente en primer lugar en todos los benchmarks y rankings. No es el salto que se esperaba con GPT-5, pero todav√≠a queda mucho a√±o por delante y seguro que OpenAI nos da m√°s sorpresas.</p>

<p>5Ô∏è‚É£ Al d√≠a siguiente del evento de OpenAI, el <strong>14 de mayo</strong>, se celebr√≥ el <strong>Google I/O</strong>, el evento de Google orientado a desarrolladores. Estaba claro que OpenAI hab√≠a contraprogramado a Google. Y con mucho √©xito, dada la repercusi√≥n de las referencias a ‚ÄúHer‚Äù y la frescura del evento, comparado con un evento de Google mucho m√°s pesado, largo y falto de foco.</p>

<p>En su <em>keynote</em>, los de Google presentaron muchos proyectos en marcha [<a href="https://arstechnica.com/information-technology/2024/05/google-strikes-back-at-openai-with-project-astra-ai-agent-prototype/">Google strikes back at OpenAI with ‚ÄúProject Astra‚Äù AI agent prototype - arstechnica.com</a> y <a href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/?utm_source=gdm&amp;utm_medium=referral&amp;utm_campaign=io24#gemini-model-updates">Gemini breaks new ground with a faster model, longer context, AI agents and more - blog.google</a>], pero pocos productos terminados. Entre las cosas m√°s interesantes se encuentra Veo, un generador de v√≠deos del estilo de Sora, una nueva versi√≥n del modelo generador de im√°genes, Imagen 3, y el proyecto Astra, un asistente controlado por la voz muy similar al de OpenAI.</p>

<p>El asistente de Google tambi√©n es multimodal y puede ver el entorno a trav√©s del m√≥vil. Todav√≠a no lo han lanzado como producto, pero presentaron un v√≠deo grabado (no en vivo c√≥mo hizo OpenAI) mostrando su funcionamiento.</p>

<div class="native-video-embed" data-component-name="VideoPlaceholder" data-attrs="{&quot;mediaUploadId&quot;:&quot;05c977d6-9ba1-41bb-97e9-f4b07a741a02&quot;,&quot;duration&quot;:null}">
</div>
<p>Aunque lo que vemos es bastante espectacular (con el giro final de ‚Äú¬ød√≥nde me he dejado mis gafas?‚Äù), la interacci√≥n y la voz del asistente no est√°n tan logradas como las del de OpenAI.</p>

<p>No han desvelado demasiadas caracter√≠sticas del modelo de lenguaje en el que est√° basada esta nueva IA. Lo √∫nico que han dicho es que se trata de un modelo multimodal (como GPT-4o), pero no han dado demasiadas especificaciones ni se ha podido probar. </p>

<p>Tambi√©n han presentado una peque√±a actualizaci√≥n del modelo ya existente, el Gemini 1.5 Pro, con el que van a permitir trabajar con contextos de hasta 2 millones de tokens (texto, im√°genes y v√≠deo). Una barbaridad. Ahora que ya est√° disponible en Europa, es el momento de probarlo y comprobar qu√© cosas puede hacer. Es verdad que no se le presta demasiada atenci√≥n (se habla mucho m√°s, por ejemplo, de Claude), pero en los rankings est√° situado muy arriba y es uno de los modelos m√°s avanzados.</p>

<p>Aunque no presentaron demasiados productos reales, el Google I/O estuvo lleno de menciones a la IA. Muy gracioso el v√≠deo que han montado los de TechCrunch y que <a href="https://x.com/TechCrunch/status/1790504691945898300">han publicado en X</a>:</p>

<div class="native-video-embed" data-component-name="VideoPlaceholder" data-attrs="{&quot;mediaUploadId&quot;:&quot;6b623eae-8666-4fe5-a20f-7f48e357d2e3&quot;,&quot;duration&quot;:null}">
</div>
<p>5Ô∏è‚É£ Hablando de <strong>rankings</strong> y de <strong>benchmarks</strong>, √∫ltimamente se han actualizado muchos de ellos. Vamos a repasarlos.</p>

<p>El primero, uno de los m√°s usados, es el MMLU, que acaba de lanzar una nueva versi√≥n m√°s avanzada denominada MMLU-pro [<a href="https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro">MMLU-Pro - huggingface.co</a>]. Se trata de una colecci√≥n de 12.000 preguntas de distintos campos (biolog√≠a, matem√°ticas, econom√≠a, inform√°tica, etc.). En la √∫ltima versi√≥n han aumentado el n√∫mero de opciones de cada pregunta a 10. Est√° realizado por el laboratorio TIGER-Lab en la Universidad de Waterloo.</p>

<p>La siguiente figura muestra los resultados de la √∫ltima evaluaci√≥n de modelos, con GPT-4o claramente en primer lugar.</p>

<p>
<img src="41375f47-6e4a-414a-ab69-1dbf70c32e89_1979x1180.png" alt="">
</p>

<p>
<a href="https://x.com/WenhuChen/status/1790597967319007564">Post en X de Wenhu Chen</a> con los resultados de la evaluaci√≥n m√°s reciente del benchmark MMLU-Pro.</p>

<p>Otro estilo de ranking es el LMSys arena [<a href="https://chat.lmsys.org/?leaderboard">LMSYS Chatbot Arena Leaderboard - lmsys.org</a>], en el que la puntuaci√≥n se obtiene a partir de los enfrentamientos entre parejas de modelos, a los que los usuarios les proporcionan preguntas, leen sus contestaciones y despu√©s escogen el ganador. Vemos que el ganador claro es tambi√©n GPT-4o (con el nombre clave que hab√≠a usado durante algunos d√≠as de im-also-a-good-gpt2-chatbot) a cierta distancia de otro grupo en cabeza formado por versiones de GPT-4, Gemini-1.5-pro y Claude-3-opus.</p>

<p>
<img src="c811fc0b-9fb6-43c4-913c-980a6773a6f6_1200x700.jpeg" alt="">
</p>

<p>Otro gr√°fico muy interesante es el que publica peri√≥dicamente Maxime Labonne en X, en el que se puede apreciar c√≥mo evolucionan los modelos en el tiempo. En el eje vertical coloca el nivel obtenido en LMSys-Arena y en el eje horizontal la fecha en que el modelo se ha lanzado. Tambi√©n marca en rojo los modelos cerrados y en verde los abiertos. De esta forma, podemos analizar c√≥mo progresan los distintos tipos de modelos.</p>

<p>
<img src="6fe32617-c940-4d96-9368-bd6f01200d23_3493x2402.jpeg" alt="">
</p>

<p>Podemos ver cosas muy curiosas, como que el modelo m√°s potente actual de Meta, Llama-3 70B, est√° al nivel del GPT-4 de hace un a√±o. Un a√±o es el tiempo que tardan en difundirse los avances de OpenAI a los modelos abiertos. Supongo que seguir√° pasando lo mismo en el futuro, y que en junio de 2025 veremos modelos abiertos similares al actual GPT-4o. Otra cosa interesante que podemos notar es c√≥mo la curva superior sigue creciendo, lo que nos lleva a la siguiente noticia.</p>

<p>6Ô∏è‚É£ Toda la industria sigue convencida de que <strong>la hip√≥tesis del escalado</strong> [<a href="https://gwern.net/scaling-hypothesis">The scaling hipothesis - gwern.net</a>] es cierta y que modelos m√°s grandes entrenados con m√°s cantidad y variedad de datos obtendr√°n resultados considerablemente mejores. Hay muchas inversiones en marcha (de Microsoft, OpenAI, Meta, etc.) en la construcci√≥n de enormes centros de procesamiento de datos, e incluso en la construcci√≥n de centrales de energ√≠a para alimentar estos centros.</p>

<p>Por ejemplo, en la interesante entrevista a Dario Amodei [<a href="https://podcasts.apple.com/es/podcast/the-ezra-klein-show/id1548604447?i=1000652234981">What if Dario Amodei Is Right About A.I. - apple.com</a>] del podcast del New York Times The Ezra Klein Show, el CEO de Anthropic se manifiesta totalmente convencido de esta hip√≥tesis de escalado y habla de cifras de 10.000 millones de d√≥lares para entrenar los modelos futuros:</p>

<blockquote>
<p>"Vamos a tener que hacer modelos m√°s grandes que utilicen m√°s capacidad de c√≥mputo por iteraci√≥n. Vamos a tener que ejecutarlos durante m√°s tiempo aliment√°ndolos con m√°s datos. Y esa cantidad de chips por el tiempo que ejecutamos cosas en ellos se traduce esencialmente en un valor monetario porque estos chips se alquilan por hora. Ese es el modelo m√°s com√∫n para ello. As√≠ que los modelos actuales cuestan del orden de 100 millones de d√≥lares para entrenar, m√°s o menos, un factor de dos o tres. Los modelos que est√°n en entrenamiento ahora, y que saldr√°n en varios momentos m√°s tarde este a√±o o a principios del pr√≥ximo, est√°n m√°s cerca de costar mil millones de d√≥lares. As√≠ que eso ya est√° ocurriendo. Y luego creo que en 2025 y 2026, nos acercaremos m√°s a cinco o diez mil millones.</p>

<p>¬øEntonces va a ser de 100 mil millones de d√≥lares? Quiero decir, muy r√°pidamente, la artiller√≠a financiera que necesitas para crear uno de estos va a excluir a cualquiera excepto a los jugadores m√°s grandes."</p>

</blockquote>

<p>Mark Zuckerberg, CEO de Meta, es algo m√°s cauto en la entrevista realizada en el podcast de Dwarkesh Patel [<a href="https://www.dwarkeshpatel.com/p/mark-zuckerberg">Mark Zuckerberg - Llama 3, Open Sourcing $10b Models, &amp; Caesar Augustus - dwarkeshpatel.com</a>]</p>

<div id="youtube2-bc6uFV9CJGg" class="youtube-wrap" data-attrs="{&quot;videoId&quot;:&quot;bc6uFV9CJGg&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM">
<div class="youtube-inner">
<iframe src="https://www.youtube-nocookie.com/embed/bc6uFV9CJGg?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409">
</iframe>
</div>
</div>
<blockquote>
<p>"Esta es una de las grandes preguntas, ¬øverdad? [el crecimiento exponencial de los resultados obtenidos por los modelos] Creo que nadie lo sabe. Una de las cosas m√°s complicadas de planificar es una curva exponencial. ¬øCu√°nto tiempo seguir√° as√≠? Creo que es lo suficientemente probable que sigamos avanzando. Creo que vale la pena invertir los 10 mil millones o m√°s de 100 mil millones de d√≥lares en construir la infraestructura y asumir que, si sigue adelante, obtendr√°s cosas realmente asombrosas que har√°n productos incre√≠bles. No creo que nadie en la industria pueda decirte con certeza que continuar√° escalando a ese ritmo. En general, en la historia, te encuentras con cuellos de botella en ciertos puntos. Ahora hay tanta energ√≠a en esto que tal vez esos cuellos de botella se superen bastante r√°pido. Creo que esa es una pregunta interesante."</p>

</blockquote>

<p>Y sobre el tama√±o de los modelos futuros y si tambi√©n los har√°n abiertos:</p>

<blockquote>
<p>"Tenemos una hoja de ruta de nuevos lanzamientos que van a traer multimodalidad, m√°s multiling√ºismo y ventanas de contexto m√°s grandes tambi√©n. Con suerte, en alg√∫n momento m√°s adelante en el a√±o podremos lanzar el modelo de 405 mil millones de par√°metros."</p>

</blockquote>

<blockquote>
<p>"Obviamente estamos muy a favor del c√≥digo abierto, pero no me he comprometido a liberar absolutamente todo lo que hacemos. B√°sicamente estoy muy inclinado a pensar que el c√≥digo abierto va a ser bueno para la comunidad y tambi√©n para nosotros porque nos beneficiaremos de las innovaciones. Sin embargo, si en alg√∫n momento hay un cambio cualitativo en lo que la cosa es capaz de hacer, y sentimos que no es responsable hacerlo de c√≥digo abierto, entonces no lo haremos. Es muy dif√≠cil de predecir."</p>

</blockquote>

<h2>üë∑‚Äç‚ôÇÔ∏è Mis treinta d√≠as</h2>

<p>A pesar de que ha pasado un mes desde la √∫ltima vez, no tengo muchas novedades que contar. Ni en libros (sigo con ‚ÄúEl bosque oscuro‚Äù; no he avanzado demasiado), ni en cosas que estoy trasteando. </p>

<p>Voy a destacar las dos pel√≠culas que m√°s me han gustado de las que hemos visto.</p>

<h3>üì∫ Dos pel√≠culas</h3>

<p>Dos pel√≠culas muy distintas pero que me han gustado un mont√≥n<a class="footnote-anchor" data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2" href="#footnote-2" target="_self">2</a>.</p>

<p>La primera <strong>Desconocidos</strong>, de <strong>Andrew Haigh</strong>, con unas enormes interpretaciones de <strong>Andrew Scott</strong> y <strong>Paul Mescal</strong>. Una pel√≠cula muy personal y profunda, que emociona y da que pensar. Y con una banda sonora llena de canciones evocadoras para los que fuimos j√≥venes en los 80.</p>

<p>
<img src="86dea0b3-54c5-4ed7-afa3-2e01371b8cf7_800x1200.jpeg" alt="">
</p>

<p>Y la segunda, el <strong>Reino del Planeta de los Simios</strong>, de <strong>Wes Ball</strong>. Una vuelta a la aventura y los temas que me parecen m√°s interesantes de la saga: la formaci√≥n de la sociedad de los simios, las relaciones con los humanos, las relaciones de poder invertidas entre simios y humanos o la reescritura de la historia por los ganadores. </p>

<p>Algunas escenas me han recordado la pel√≠cula original de Charlton Heston y me ha dado la sensaci√≥n de que la historia se va moviendo en esa direcci√≥n. ¬°Quiero m√°s!</p>

<p>
<img src="7b58adec-d6b4-4c60-862d-6ebe33c7e49f_809x1200.jpeg" alt="">
</p>

<p>Y con esto terminamos por este mes. ¬°Hasta la pr√≥xima quincena, nos leemos! üëãüëã</p>

<div class="footnote" data-component-name="FootnoteToDOM">
<a id="footnote-1" href="#footnote-anchor-1" class="footnote-number" contenteditable="false" target="_self">1</a>
<div class="footnote-content">
<p>No existe en la actualidad ning√∫n modelo de lenguaje que haya sido entrenado con secuencias de v√≠deo. No existe en la actualidad potencia computacional para llevar a cabo tal entrenamiento. Quiz√°s se har√° con GPT-6 o GPT-7 y eso s√≠ que ser√° revolucionario, porque un modelo de este tipo podr√° aprender y generalizar el funcionamiento del mundo real, la f√≠sica impl√≠cita en los movimientos de los objetos. Y, a su vez, podr√° aplicar estos conceptos a las otras modalidades (por ejemplo, podr√° entender mejor las relaciones espaciales cuando hablamos de lejos/cerca o izquierda/derecha).</p>

</div>
</div>
<div class="footnote" data-component-name="FootnoteToDOM">
<a id="footnote-2" href="#footnote-anchor-2" class="footnote-number" contenteditable="false" target="_self">2</a>
<div class="footnote-content">
<p>Mi hija Luc√≠a me va dar un tir√≥n de orejas por no poner <strong>Rivales</strong>, pero ella ya sabe por qu√© es üòÇ. Prometo comentar la pel√≠cula la pr√≥xima vez que la vea ‚ù§Ô∏è.</p>

</div>
</div>
