---
title: "Donde dije \"sentido com√∫n\" digo \"razonamiento\" (#17 de 2024)"
date: 2024-10-25
draft: false
tags:
  - "newsletter"
---
<p>Hoy toca un art√≠culo casi de opini√≥n. Vamos a reflexionar sobre c√≥mo, en el debate sobre la inteligencia artificial, hemos pasado de hablar de ‚Äúsentido com√∫n‚Äù a centrarnos en el ‚Äúrazonamiento‚Äù. Exploraremos esta evoluci√≥n y discutiremos c√≥mo los modelos de lenguaje actuales est√°n desafiando las nociones tradicionales de comprensi√≥n y pensamiento.</p>

<p>¬°Gracias por leerme!</p>

<p>
<img src="Pasted image 20241025084906.png" alt="">
</p>

<p>Imagen generada por ChatGPT con el prompt: ‚Äúgenera una imagen que muestre un robot pensando‚Äù.</p>

<p>Estos √∫ltimos d√≠as he estado leyendo dos libros muy recomendables sobre inteligencia artificial: <em>
<a href="https://www.goodreads.com/book/show/210808890-inteligencia-artificial">Inteligencia Artificial: Gu√≠a para seres pensantes</a>
</em> de <strong>
<a href="https://melaniemitchell.me">Melanie Mitchell</a>
</strong> y <em>
<a href="https://www.goodreads.com/book/show/208882425-artificial-intelligence">Artificial Intelligence: 10 Things You Should Know</a>
</em> de <strong>
<a href="https://rockt.github.io">Tim Rockt√§schel</a>
</strong>. </p>

<p>
<img src="1b8a0e69-5e28-4011-8bdd-4b95330706dc_1408x1080.png" alt="">
</p>

<p>Son libros bastante distintos. El primero es m√°s profundo y extenso, se public√≥ en 2020 y hace bastante √©nfasis en los problemas de los algoritmos de IA para alcanzar una <strong>compresi√≥n del mundo</strong> similar a la de los humanos. El segundo es m√°s breve, est√° escrito en forma de peque√±os ensayos, se ha publicado recientemente y presenta una visi√≥n muy optimista respecto a las posibilidades de los actuales LLMs para llegar a ser los elementos centrales de una futura IA con capacidades que exceder√°n a las humanas.</p>

<p>Ambos libros est√°n escritos por excelentes investigadores y se complementan estupendamente. El primer libro nos da una visi√≥n cautelosa y muy bien razonada sobre la dificultad de construir algoritmos inteligentes generales (lo que ahora llamamos <a href="/posts/agi-o-no-agi-14-de-2024/">AGI</a>), mientras que el segundo nos muestra c√≥mo los avances de los √∫ltimos a√±os representan un avance importante que nos puede llevar en poco tiempo a sistemas con capacidades superiores a los humanos (y lo explica de una forma muy comprensible y razonada, no como lo hace <a href="https://es.wikipedia.org/wiki/Nick_Bostrom">alg√∫n otro</a>).</p>

<h2>Sentido com√∫n</h2>

<p>Uno de los problemas que plantea <strong>Mitchell</strong> en su libro es el problema del sentido com√∫n. Es un problema que ha estado presente en el campo de la IA desde sus comienzos. </p>

<p>El problema del ‚Äúsentido com√∫n‚Äù en inteligencia artificial, especialmente en el √°mbito del procesamiento del lenguaje natural, se refiere a la dificultad que tienen las m√°quinas para interpretar y generar lenguaje de manera coherente con el <strong>conocimiento impl√≠cito</strong> que los humanos empleamos habitualmente. Este tipo de conocimiento incluye la capacidad de entender situaciones ambiguas, inferir significados ocultos, realizar suposiciones sobre el contexto o incluso captar intenciones y emociones impl√≠citas.</p>

<p>En su libro, <strong>Mitchell</strong> pone un ejemplo de una descripci√≥n de una situaci√≥n sobre una persona que va a un restaurante. Veamos otro ejemplo similar:</p>

<blockquote>
<p>‚ÄúSara agit√≥ la varita y toc√≥ con ella el juguete del ni√±o, que dej√≥ de llorar al comprobar que volv√≠a a funcionar. Cuando Sara lleg√≥ a casa y le cont√≥ a su padre lo que hab√≠a pasado, este se enfad√≥ much√≠simo y la castig√≥ sin salir de casa durante una semana.‚Äù</p>

</blockquote>

<p>Para entender la situaci√≥n planteada por la descripci√≥n anterior hay que comprender much√≠simas cosas (adem√°s de haber le√≠do los libros de Harry Potter): Sara lleva la varita en la mano, el juguete estaba roto y por eso el ni√±o estaba llorando, Sara vio al ni√±o llorar y quiso ayudarle, Sara hizo algo m√°gico con la varita, Sara estaba orgullosa por haber ayudado al ni√±o, etc.</p>

<p>¬øPuede una IA  llegar a hacer estas suposiciones?</p>

<p>
<strong>Mitchell</strong> dice:</p>

<blockquote>
<p>Aunque el procesamiento autom√°tico del lenguaje natural ha avanzado mucho, no creo que las m√°quinas puedan llegar a comprender por completo el lenguaje humano hasta que tengan un sentido com√∫n similar al humano.</p>

</blockquote>

<p>Y los sistemas que exist√≠an en 2020 (la fecha en la que public√≥ el libro) no ten√≠an esa competencia:</p>

<blockquote>
<p>Los humanos tienen una competencia fundamental de la que carecen los sistemas actuales de IA: el sentido com√∫n.</p>

</blockquote>

<h2>Llegan los LLMs</h2>

<p>¬øCu√°l es la situaci√≥n en la actualidad? ¬øHan adquirido los LLMs este sentido com√∫n?</p>

<p>En los √∫ltimos a√±os se han entrenado con todo internet estos gigantescos sistemas para <strong>predecir la siguiente palabra</strong> de un texto. La propia <strong>Mitchell</strong> explica el funcionamiento de los LLMs en <a href="https://oecs.mit.edu/pub/zp5n8ivs/release/1?readingCollection=9dd2a47d">un art√≠culo</a> reciente. El sencillo objetivo de predecir la siguiente palabra produce resultados sorprendentes y dota a estos sistemas de enormes capacidades.</p>

<p>La raz√≥n es que una IA que aprende a comprimir textos con el objetivo de predecir la siguiente palabra tiene que aprender una cantidad enorme de datos y de conceptos subyacentes en los textos originales. Por ejemplo, cuando el LLM aprende la palabra que sigue a la frase:</p>

<p>‚ÄúHarry estaba de excursi√≥n fuera de Howarts y us√≥ su varita m√°gica. Cuando los profesores se enteraron, decidieron ..... (castigarle)" </p>

<p>est√° aprendiendo al mismo tiempo una cantidad enorme de conceptos: existen reglas que limitan el uso de la magia, una regla es que la magia solo se puede usar en Howarts, si no se cumple una regla se debe castigar, los profesores son los que deciden sobre el comportamiento de los estudiantes, etc. </p>

<p>La famosa <strong>
<a href="https://gwern.net/scaling-hypothesis">hip√≥tesis de escalado</a>
</strong> argumenta que, cuanto mayores son los par√°metros de los LLMs y m√°s diversos son los datos de entrada, mayores abstracciones son capaces de aprender los sistemas. Al optimizar la funci√≥n objetivo, los <em>
<a href="https://platform.openai.com/docs/guides/embeddings">embeddings</a>
</em>que representan los tokens del lenguaje se colocan en posiciones √≥ptimas del espacio de par√°metros y los pesos de la red neuronal del LLM se configuran para representar funciones de transformaci√≥n que reflejan <strong>relaciones y abstracciones de mayor nivel</strong>.</p>

<p>Realmente, esto es solo una teor√≠a y existen muchas inc√≥gnitas sobre hasta d√≥nde es posible escalar y qu√© tipo de datos de entrada son necesarios para conseguir mayores abstracciones. Pero el nivel de comprensi√≥n del sentido com√∫n alcanzado por los sistemas actuales es sorprendente.</p>

<p>Por ejemplo, veamos qu√© responde ChatGPT (4o) cuando le pasamos el texto anterior, sobre Sara y la varita m√°gica, y le preguntamos por las razones por las que el padre le ha castigado.</p>

<blockquote>
<p>
<strong>Yo:</strong>
</p>

<p>Lee el siguiente texto y contesta a la pregunta a continuaci√≥n:</p>

<p>‚ÄúJuan agit√≥ la varita y toc√≥ con ella el juguete del ni√±o, que dej√≥ de llorar al comprobar que volv√≠a a funcionar. Cuando Juan lleg√≥ a casa y le cont√≥ a su padre lo que hab√≠a pasado, este se enfad√≥ much√≠simo y lo castig√≥ a no salir de casa durante una semana.‚Äù</p>

<p>Pregunta: ¬øPor qu√© se enfad√≥ el padre de Juan?</p>

<p>
<strong>ChatGPT:</strong> &nbsp;</p>

<p>El padre de Juan se enfad√≥ probablemente porque Juan us√≥ la varita de una forma que su padre no aprobaba, lo que podr√≠a indicar que el uso de la varita estaba prohibido o era peligroso.</p>

<p>
<strong>Yo:</strong>
</p>

<p>Dime cuatro posibles razones distintas que expliquen el enfado el padre de Juan</p>

<p>
<strong>ChatGPT:</strong>
</p>

<p>Aqu√≠ tienes cuatro posibles razones alternativas que podr√≠an explicar el enfado del padre de Juan:</p>

<p>1. <strong>Juan utiliz√≥ una habilidad prohibida</strong>: Es posible que el uso de la varita est√© prohibido en su casa o que sea una habilidad m√°gica que Juan no deber√≠a haber utilizado fuera de ciertos contextos.</p>

<p>2. <strong>El padre teme las consecuencias</strong>: El padre podr√≠a estar preocupado de que el uso de la varita en p√∫blico atraiga atenci√≥n no deseada o peligros para la familia, como si fuese algo secreto o peligroso.</p>

<p>3. <strong>Juan rompi√≥ una regla familiar</strong>: Quiz√°s en la familia hay una regla estricta sobre cu√°ndo y c√≥mo utilizar la magia o herramientas especiales, y Juan la desobedeci√≥ al arreglar el juguete del ni√±o sin permiso.</p>

<p>4. <strong>El padre teme por la seguridad de Juan</strong>: El padre podr√≠a haberse enfadado porque, aunque el gesto de Juan fue bienintencionado, usar la varita podr√≠a poner en peligro la integridad de Juan, como si existiera un riesgo asociado a su uso.</p>

</blockquote>

<p>Es mejorable, las explicaciones 1 y 3 son algo redundantes, pero da la sensaci√≥n de que el sistema ‚Äúentiende‚Äù perfectamente conceptos como prohibido, reglas, seguridad o riesgo. </p>

<p>Se ha avanzado much√≠simo en estos √∫ltimos 4 a√±os.</p>

<h2>Ahora el t√©rmino del debate es ‚Äúrazonamiento‚Äù</h2>

<p>A pesar de estos avances, muchos investigadores del campo de la IA, la filosof√≠a o la neurociencia <strong>niegan</strong> que lo que hacen los LLMs se pueda denominar ‚Äúcomprender‚Äù. <strong>Mitchell</strong> explica muy bien esta postura en su art√≠culo de 2023, <em>
<a href="https://arxiv.org/abs/2210.13966">‚ÄúThe Debate Over Understanding in AI's Large Language Models‚Äù</a>
</em>, en el que presenta con gran detalle los argumentos de los que est√°n a favor y en contra.</p>

<p>Y termina aceptando que los LLMs han aprendido a manejar (alguna forma de) sentido com√∫n:</p>

<blockquote>
<p>Podr√≠a argumentarse que, en los √∫ltimos a√±os, el campo de la IA ha creado m√°quinas con nuevos modos de comprensi√≥n. Los problemas que requieren grandes cantidades de conocimiento seguir√°n favoreciendo a los modelos estad√≠sticos a gran escala como los LLM.</p>

</blockquote>

<p>Pero a√±ade un detalle importante, la nueva gran cr√≠tica. <strong>Lo que no pueden hacer es razonar y planificar</strong>. Eso est√° limitado a la inteligencia humana:</p>

<blockquote>
<p>Pero aquellos problemas para los que tenemos un conocimiento limitado y mecanismos causales s√≥lidos favorecer√°n a la inteligencia humana. </p>

</blockquote>

<p>Cuando <strong>Mitchell</strong> habla de ‚Äúmecanismos causales s√≥lidos‚Äù y ‚Äúconocimiento limitado‚Äù se est√° refiriendo a nuestra capacidad de planificaci√≥n y razonamiento. Por ejemplo, para planificar un viaje usando la web, una persona puede encontrar vuelos y hoteles, pero tambi√©n debe considerar factores como la hora de llegada y la disponibilidad de transporte. Si llega tarde por la noche y no hay transporte p√∫blico, buscar√° un hotel cerca del aeropuerto. </p>

<p>Este tipo de razonamiento causal, ajustando el plan seg√∫n condiciones impredecibles, y realizando varios pasos de inferencia, para muchos autores <strong>no es posible de conseguir con los actuales LLMs basados en autorregresi√≥n</strong>.</p>

<p>La propia <strong>Mitchell</strong> incide en el tema en varios posts recientes:</p>

<ul>
<li>
<p>
<a href="https://aiguide.substack.com/p/can-large-language-models-reason">Can Large Language Models Reason?</a> (Sep, 2023)</p>

</li>
<li>
<p>
<a href="https://aiguide.substack.com/p/the-llm-reasoning-debate-heats-up">The LLM Reasoning Debate Heats Up</a> (Oct, 2024)</p>

</li>
</ul>

<p>El debate se ha intensificado con el lanzamiento de <strong>o1</strong>, un modelo que, seg√∫n <strong>OpenAI</strong>
<a href="/posts/como-funciona-o1-15-de-2024/">ha sido construido precisamente para razonar</a>. Por ejemplo, se ha hablado mucho en los √∫ltimos d√≠as en X del <a href="https://arxiv.org/abs/2410.05229">art√≠culo de unos investigadores de Apple</a> en el que enga√±an a distintos LLMs a√±adiendo datos irrelevantes a enunciados de problemas de primaria. Estuve haciendo alg√∫n <a href="https://x.com/domingogallardo/status/1845769109100749286">experimento r√°pido</a> y me dio la sensaci√≥n de que o1 no sufre este problema, pero habr√° que investigar m√°s.</p>

<h2>Los LLMs todav√≠a tienen mucho recorrido</h2>

<p>La revoluci√≥n que ha supuesto la aplicaci√≥n del <em>deep learning</em> a los problemas del procesamiento del lenguaje plantea una gran inc√≥gnita de cara al futuro. ¬øHasta d√≥nde se podr√° escalar esta tecnolog√≠a? ¬øPodremos conseguir con ella <strong>agentes inteligentes</strong> capaces de interactuar con nuestros datos y con la web y ayudarnos en tareas relevantes? ¬øPodr√°n construirse agentes a los que podamos encargar tareas que le <strong>ocupen horas o d√≠as</strong>, en las que tenga que ir obteniendo informaci√≥n paso a paso, realizando experimentos y obteniendo resultados?</p>

<p>Todav√≠a es pronto para saberlo. El crecimiento de las capacidades de los LLMs ha sido hasta ahora exponencial, pero no sabemos si esta tendencia continuar√° o si estamos alcanzando un punto de inflexi√≥n, donde el crecimiento podr√≠a estabilizarse siguiendo una curva log√≠stica (una curva en forma de ‚ÄúS‚Äù que se aplana al llegar a un l√≠mite).</p>

<p>Tambi√©n puede ser que lo que se necesite sea<strong> combinar LLMs o afinar los datos de aprendizaje</strong>. OpenAI, con la construcci√≥n de <strong>o1 </strong>basado en un paradigma algo diferente (aunque no deja de ser un LLM), demuestra que es posible construir nuevos sistemas basados en los actuales. Investigadores como el anteriormente mencionado <strong>Tim Rockt√§schel </strong>argumentan que las funcionalidades de los actuales LLMs de generar alternativas y validarlas pueden ser la base de sistemas capaces de <strong>mejorarse a s√≠ mismos</strong>. El propio <strong>Fran√ßois Chollet</strong>, que hemos mencionado m√°s de una vez en esta <em>newsletter</em>, dice que los LLMs, con su m√©todo basado en la memorizaci√≥n de patrones, todav√≠a pueden conseguir muchas m√°s cosas.</p>

<p>
<img src="d9d9f462-381a-44c8-a2cd-5b7b95006b42_1190x730.png" alt="">
</p>

<h2>No te dediques a los LLMs</h2>

<p>Otros investigadores son mucho m√°s cr√≠ticos con la tecnolog√≠a actual. Por ejemplo <strong>Yann LeCun</strong> argumenta que es necesario ampliar los actuales tokens con elementos que combinen texto, v√≠deo, acciones y otros datos sensoriales tomados del mundo real. Propone una arquitectura tambi√©n basada en aprendizaje de <em>embeddings</em>, pero de una forma radicalmente distinta a los actuales LLMs, denominada <strong>JEPA</strong> (Joint Embedding Predictive Architecture).</p>

<div id="youtube2-xL6Y0dpXEwc" class="youtube-wrap" data-attrs="{&quot;videoId&quot;:&quot;xL6Y0dpXEwc&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM">
<div class="youtube-inner">
<iframe src="https://www.youtube-nocookie.com/embed/xL6Y0dpXEwc?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409">
</iframe>
</div>
</div>
<p>En esta √∫ltima charla, incluso hace la recomendaci√≥n a los j√≥venes investigadores de que no se dediquen a los LLMs (si lo que quieren es descubrir cosas relevantes y no solo ganar dinero) y hace la siguiente predicci√≥n: en los pr√≥ximos 2-3 a√±os, los esfuerzos de los actuales centros de datos gigantes <strong>no van a dar resultados</strong> y se va a dejar de hablar de "escalado". Los LLMs ser√°n un elemento de la soluci√≥n, pero no el elemento fundamental.</p>

<p>Pero no nos equivoquemos con la posici√≥n de <strong>LeCun</strong>. No est√° en el lado de quienes piensan que los computadores nunca podr√°n pensar como los humanos, todo lo contrario, plantea que la AGI es posible, aunque sea con una tecnolog√≠a distinta de la actual. Incluso en la charla anterior menciona un plazo de una d√©cada (supongo que para <a href="https://x.com/tsarnick/status/1846084308416843787">darle una respuesta a su jefe, Zuck</a>).</p>

<h2>Cr√≠ticos y apocal√≠pticos</h2>

<p>Frente a esta posici√≥n, la corriente m√°s cr√≠tica de la IA niega incluso que estemos viendo avances hacia la inteligencia humana. Parecen aplicar el ‚Äúteorema <a href="https://en.wikipedia.org/wiki/Larry_Tesler">Tesler</a>‚Äù o el <a href="https://en.wikipedia.org/wiki/AI_effect">efecto IA</a>:</p>

<blockquote>
<p>La inteligencia es todo lo que las m√°quinas todav√≠a no pueden hacer.</p>

</blockquote>

<p>Seg√∫n ellos, la inteligencia es compleja, multifuncional y est√° profundamente relacionada con otros elementos intr√≠nsecamente humanos como el pensamiento, la cognici√≥n, las emociones y la consciencia. </p>

<p>A pesar de numerosas evidencias, siguen negando que estos modelos hayan conseguido ni siquiera un poco de comprensi√≥n. Da la sensaci√≥n de que tienen miedo de que una m√°quina pueda llegar a ser inteligente. </p>

<p>La an√©cdota que recoge <strong>Mitchell</strong> al principio de su libro es muy reveladora. <strong>Douglas Hofstadter</strong>, el conocido investigador de IA, autor del c√©lebre libro <a href="https://www.goodreads.com/book/show/49401016-godel-escher-y-bach">G√∂del, Escher y Bach: Un Eterno y Gr√°cil Bucle</a>, termin√≥ diciendo en una reuni√≥n en 2014 en Google:</p>

<blockquote>
<p>Estoy aterrorizado. Me parece aterrador, muy preocupante, muy triste. Nos sustituir√°n. Seremos reliquias, nos dejar√°n tirados.</p>

</blockquote>

<p>
<strong>Mitchell</strong> explica despu√©s ese terror:</p>

<blockquote>
<p>El terror de <strong>Hofstadter</strong> era que la inteligencia, la creatividad, las emociones e incluso la propia consciencia fueran demasiado f√°ciles de crear, que los aspectos de la humanidad que m√°s valiosos le parec√≠an acabaran siendo una mera ‚Äúserie de trucos‚Äù, que un conjunto superficial de algoritmos de fuerza bruta pudiera explicar el esp√≠ritu humano.</p>

</blockquote>

<p>Creo que versiones parecidas a esta idea son las que llevan a estos investigadores cr√≠ticos a relativizar cualquier avance que se produce. Parad√≥jicamente, creo que tambi√©n son estas mismas ideas las que alarman a los ‚Äúapocal√≠pticos‚Äù como <strong>Geoffrey Hinton</strong>. En el fondo temen que nuestra humanidad no sea m√°s que<strong> un algoritmo de fuerza bruta aprendido y capturado en miles de millones de par√°metros</strong>.</p>

<h2>Una inteligencia distinta</h2>

<p>Cuando empec√© a escribir este art√≠culo, no quer√≠a hacerlo demasiado largo. Pero, como siempre pasa, una cosa me ha llevado a otra, y al final he terminado abriendo un mel√≥n que quer√≠a dejar para otro d√≠a, el de la consciencia o, por ser m√°s espec√≠fico, el de la ‚Äúsentiencia‚Äù (traducci√≥n literal, creo que no aceptada, del t√©rmino ingl√©s ‚Äúsentience‚Äù).</p>

<p>Dejo toda la argumentaci√≥n para otro art√≠culo, pero creo que la forma de escapar de este terror es considerar que lo que nos separa de los LLMs es exactamente lo mismo que nos relaciona con muchos otros seres vivos: <strong>la posibilidad de experimentar sensaciones, dolor, placer, miedo o alegr√≠a</strong>. Los algoritmos actuales (y, en mi opini√≥n, cualquier algoritmo futuro) no son capaces de sentir. </p>

<p>Esto nos libera de much√≠simos <strong>problemas √©ticos que s√≠ tenemos</strong> con nuestros parientes, como mam√≠feros, vertebrados e incluso invertebrados m√°s complejos. A diferencia de los algoritmos, estos seres poseen una capacidad de sentir dolor, placer y otros estados emocionales, lo que nos obliga a considerar su bienestar y sus derechos en nuestras decisiones √©ticas.</p>

<p>La falta de sentiencia en las m√°quinas nos permite considerarlas como meras ‚Äúm√°quinas pensantes‚Äù no sintientes y nos permite aceptar una visi√≥n en la que ‚ÄúAGI‚Äù no equivale a ‚Äúhumano‚Äù.</p>

<p>Termino con la cita de <strong>Mitchell</strong> con la que tambi√©n concluye su art√≠culo sobre el debate acerca de la comprensi√≥n:</p>

<blockquote>
<p>El desaf√≠o para el futuro es desarrollar nuevos m√©todos cient√≠ficos que puedan revelar los mecanismos detallados de comprensi√≥n en formas de inteligencia distintas, discernir sus fortalezas y limitaciones, y aprender a integrar esos modos de cognici√≥n verdaderamente diversos.</p>

</blockquote>

<div>
<hr>

</div>
<p>¬°Hasta la pr√≥xima quincena, nos leemos! üëãüëã</p>

<p>
</p>
