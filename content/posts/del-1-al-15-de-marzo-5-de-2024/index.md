---
title: "Del 1 al 15 de marzo (#5 de 2024)"
date: 2024-03-16
draft: false
tags:
  - "newsletter"
---
<p>üëãüëã ¬°Hola, soy Domingo!</p>

<p>En la primera quincena de marzo Anthropic ha lanzado el primer modelo de lenguaje que, en mi opini√≥n, es comparable (e incluso supera) a GPT-4. Y SpaceX ha conseguido poner en √≥rbita la gigantesca Starship, el cohete que abaratar√° en √≥rdenes de magnitud el coste del env√≠o de sat√©lites al espacio y que volver√° a llevar astronautas a la luna.</p>

<p>
<img src="8a5c70c5-a354-4ad2-90d4-82d9a074423c_1856x1064.png" alt="">
</p>

<p>Impresionante el v√≠deo de la Starship reentrando en la atm√≥sfera. Se puede ver el plasma a alta temperatura producido por la fricci√≥n. Poco despu√©s la nave se descontrol√≥ y estall√≥.</p>

<p>¬°Muchas gracias por leerme!</p>

<h2>üóû Noticias</h2>

<p>
</p>

<p>1Ô∏è‚É£ Comenzamos con el vuelo de <a href="https://www.spacex.com/launches/mission/?missionId=starship-flight-3">prueba de la Starship</a> de hace dos d√≠as, el 14 de marzo. En esta tercera prueba Space X ha conseguido con √©xito <strong>poner en √≥rbita la Starship</strong>. </p>

<p>La Starship es el veh√≠culo de lanzamiento reutilizable de pr√≥xima generaci√≥n de SpaceX, dise√±ado para transportar humanos y cargas √∫tiles a la √≥rbita terrestre, la Luna y Marte. La Starship promete revolucionar el acceso al espacio, rebajando el coste por kilogramo para √≥rbita terrestre de unos $3,000 a unos $100 o incluso $10. La Starship puede llevar una carga √∫til de 100 a 150 toneladas, multiplicando por m√°s de 5 la carga √∫til de los Falcon 9, la nave que usa SpaceX en la actualidad.</p>

<p>
<img src="6701fe6f-5def-4d7a-ace3-851da4254ca6_3121x2160.jpeg" alt="">
</p>

<p>Lanzamiento de la Starship.</p>

<p>Quedan todav√≠a varias pruebas en las que se deben conseguir hitos todav√≠a no alcanzados:</p>

<ul>
<li>
<p>Recuperar el Super Heavy que impulsa la Starship, haciendo que vuelva a tierra como ya es normal ver a los Falcon 9.</p>

</li>
<li>
<p>Encender en √≥rbita los motores Raptor de la Starship.</p>

</li>
<li>
<p>Reentrar y recuperar la Starship. En esta prueba no lleg√≥ a realizar la reentrada completa, explot√≥ cuando estaba comenzando a entrar en la atm√≥sfera.</p>

</li>
</ul>

<p>Un buen resumen de la prueba es, como siempre, <a href="https://danielmarin.naukas.com/2024/03/14/tercer-lanzamiento-del-sistema-starship-la-s28-se-destruye-en-la-reentrada/">el art√≠culo de Daniel Mar√≠n</a>.  Por ahora <a href="https://en.wikipedia.org/wiki/SpaceX_Starship_flight_tests">hay previstos</a> tres vuelos m√°s de prueba para este a√±o, aunque Elon Musk habla de hasta seis nuevos lanzamientos. Ir√© contando por aqu√≠ los resultados.</p>

<p>2Ô∏è‚É£ Una nueva <strong>entrevista de Lex Fridman a <a href="http://yann.lecun.com">Yann LeCun</a>
</strong>, el director de IA de Meta. LeCunn es uno de los cient√≠ficos pioneros y m√°s reconocidos en el campo del Deep Learning y las redes neuronales. Desde <a href="https://ai.meta.com/people/yann-lecun/">su puesto en Meta</a>, LeCun tiene una enorme influencia en la evoluci√≥n futura de la industria de los LLMs, sobre todo por su postura a favor de los modelos abiertos, como la <a href="https://llama.meta.com">familia de modelos LLaMA</a> (Large Language Model Meta AI). </p>

<p>LeCun argumenta que un acceso abierto a los LLMs permite una mayor colaboraci√≥n, experimentaci√≥n, transparencia y seguridad. Adem√°s, permite adaptarlos a distintas sensibilidades y culturas, permitiendo una diversidad y riqueza de modelos. Seg√∫n √©l, esta es la √∫nica forma de combatir los inevitables sesgos asociados a los modelos propietarios creados por unas pocas poderosas empresas.</p>

<div id="youtube2-5t1vTLU7s40" class="youtube-wrap" data-attrs="{&quot;videoId&quot;:&quot;5t1vTLU7s40&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM">
<div class="youtube-inner">
<iframe src="https://www.youtube-nocookie.com/embed/5t1vTLU7s40?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409">
</iframe>
</div>
</div>
<p>La entrevista es s√∫per interesante, empezando fuerte, con respuestas muy t√©cnicas en las que se mencionan enfoques alternativos a los LLMs auto regresivos. Seg√∫n LeCunn, los modelos actuales no son suficientes para conseguir una inteligencia similar a la humana, son necesarios enfoques nuevos como la <a href="https://arxiv.org/abs/2403.00504">arquitectura JEPA</a> (Joint-Embedding Predictive Architecture). Despu√©s, la entrevista gira hacia cuestiones m√°s generales relacionadas con el futuro impacto en la sociedad de la IA y de los modelos abiertos.</p>

<p>Algunos fragmentos.</p>

<p>Sobre los asistentes inteligentes:</p>

<blockquote>
<p>La IA b√°sicamente amplificar√° la inteligencia humana. Es como si cada uno de nosotros tuviera un equipo de asistentes de IA inteligentes. Podr√≠an ser m√°s inteligentes que nosotros. Har√°n lo que les pidamos, quiz√°s ejecuten una tarea de maneras que son mucho mejores de las que nosotros podr√≠amos hacerlo, porque ser√≠an m√°s inteligentes que nosotros. Y as√≠, es como si todos fu√©ramos el jefe de un equipo de personas virtuales s√∫per inteligentes. Por lo tanto, no deber√≠amos sentirnos amenazados por esto m√°s de lo que deber√≠amos sentirnos amenazados por ser el gerente de un grupo de personas, algunas de las cuales son m√°s inteligentes que nosotros. Ciertamente tengo mucha experiencia en esto, de tener personas trabajando conmigo que son m√°s inteligentes que yo.</p>

</blockquote>

<p>Sobre la IA como algo similar a la invenci√≥n de la imprenta:</p>

<blockquote>
<p>La IA va a hacer que la humanidad sea m√°s inteligente. Un evento equivalente en la historia de la humanidad a lo que podr√≠a ser proporcionado por la generalizaci√≥n de asistentes de IA es la invenci√≥n de la imprenta. Hizo que todos fueran m√°s inteligentes, el hecho de que la gente pudiera tener acceso a los libros. Los libros eran mucho m√°s baratos de lo que eran antes, y as√≠ mucha m√°s gente ten√≠a el incentivo de aprender a leer, lo cual no era el caso antes. Y la gente se volvi√≥ m√°s inteligente. Esto propici√≥ la Ilustraci√≥n. No habr√≠a habido Ilustraci√≥n sin la imprenta. Facilit√≥ la filosof√≠a, el racionalismo, el abandono de la doctrina religiosa, la democracia, la ciencia.</p>

</blockquote>

<p>Sobre la AGI:</p>

<blockquote>
<p>La IA general (AGI) no va a ser un evento. La idea, de alguna manera popularizada por la ciencia ficci√≥n y Hollywood, de que alguien va a descubrir el secreto de la AGI, y luego encender una m√°quina y entonces tendremos AGI, simplemente no va a suceder. No va a ser un evento. Va a ser un progreso gradual. ¬øVamos a tener sistemas que puedan aprender de video c√≥mo funciona el mundo y aprender buenas representaciones? S√≠. Antes de que los llevemos a la escala y rendimiento que observamos en los humanos, va a tomar bastante tiempo. No va a suceder en un d√≠a. ¬øVamos a tener sistemas que puedan tener una gran cantidad de memoria asociada para que puedan recordar cosas? S√≠, pero igual, no va a suceder ma√±ana. Hay algunas t√©cnicas b√°sicas que necesitan ser desarrolladas. Tenemos muchas de ellas, pero hacer que esto funcione junto con un sistema completo es otra historia.</p>

</blockquote>

<p>Sobre los <em>AI doomers</em> (apocal√≠pticos de la IA):</p>

<blockquote>
<p>Los<em> AI doomers</em> imaginan todo tipo de escenarios catastr√≥ficos sobre c√≥mo la IA podr√≠a escapar o controlar y b√°sicamente matarnos a todos, y eso se basa en un mont√≥n de suposiciones que son mayormente falsas. As√≠ que la primera suposici√≥n es que la emergencia de la superinteligencia va a ser un evento, que en alg√∫n momento vamos a descubrir el secreto y encenderemos una m√°quina que sea superinteligente, y porque nunca lo hab√≠amos hecho antes, va a tomar control del mundo y matarnos a todos. Eso es falso. No va a ser un evento. Vamos a tener sistemas que sean tan inteligentes como un gato, que tengan todas las caracter√≠sticas de la inteligencia a nivel humano, pero su nivel de inteligencia ser√≠a como el de un gato o un loro tal vez o algo as√≠. Luego vamos a trabajar para hacer que esas cosas sean m√°s inteligentes. A medida que los hagamos m√°s inteligentes, tambi√©n vamos a ponerles algunas barreras de seguridad y aprender c√≥mo poner algunas barreras de seguridad para que se comporten adecuadamente.</p>

</blockquote>

<p>3Ô∏è‚É£ Anthropic ha presentado una <strong>nueva familia de modelos Claude 3</strong>. En su <a href="https://www.anthropic.com/news/claude-3-family">nota informativa</a> explican sus caracter√≠sticas y sus nombres curiosos: Haiku, Sonnet y Opus. Opus es el m√°s potente.</p>

<p>Opus Se puede probar en la <a href="https://console.anthropic.com/dashboard">consola</a> que tienen para interactuar con su API. A diferencia de lo que paso con Gemini, que me decepcion√≥ enormemente, me parece un modelo que compite muy bien con GPT-4. Incluso me parece m√°s cercano y "humano" que el de OpenAI, que cada vez parece m√°s r√≠gido y formal (seguro que por culpa de todas los ajustes que le han hecho para evitar cr√≠ticas y sesgos). </p>

<p>En la tabla que presentan comparando los modelos con los ya existentes se ve que Claude 3 Opus supera en algunos tests a GPT-4. Y el modelo m√°s peque√±o, Haiku, supera a GPT-3.5. Un gran avance.</p>

<p>
<img src="Pasted image 20240309104003.png" alt="">
</p>

<p>He puesto a prueba los modelos con un test muy sencillo en el que tienen que predecir el resultado de unas acciones sobre unas figuras. El resultado confirma que Opus lo que comenta Anthropic de que el modelo es comparable con GPT-4. Explico m√°s adelante el experimento, en el apartado de "Mis quince d√≠as".</p>

<p>4Ô∏è‚É£ Me gusta el estilo bastante abierto de Anthropic sobre los <strong>prompts que usan para Claude 3</strong>. Por ejemplo, Amanda Askell, una de las ingenieras de Anthropic, <a href="https://x.com/AmandaAskell/status/1765207842993434880?s=20">ha compartido en X</a> el prompt del sistema que incluyen al comienzo de todas las interacciones. El nivel de abstracci√≥n que tiene es muy elevado, con frases como:</p>

<blockquote>
<p>Debes dar respuestas concisas a preguntas muy sencillas, pero proporcionar respuestas detalladas a preguntas m√°s complejas y abiertas.</p>

</blockquote>

<p>O esta para que siempre intente tener un punto de vista lo m√°s objetivo posible, pero sin caer en intentar dar la raz√≥n a los dos lados:</p>

<blockquote>
<p>Si te pregunta sobre un tema controvertido, debes intentar proporcionar reflexiones cuidadosas e informaci√≥n objetiva sin minimizar su contenido da√±ino o implicar que hay perspectivas razonables en ambos lados.</p>

</blockquote>

<p>En <a href="https://x.com/alexalbert__/status/1765118192291393959?s=20">otro hilo en X</a>, el ingeniero  de prompting de Antrhropic Alex Albert le pide a Opus que haga un autoretrato y le va diciendo varias veces que lo intente hacer m√°s complicado, con prompts como:</p>

<blockquote>
<p>‚Äú¬°Est√° bien! Pero quiero que intentes hacerlo todav√≠a mejor.‚Äù</p>

</blockquote>

<p>O:</p>

<blockquote>
<p>"Wow lo est√°s haciendo genial! Pero s√© que eres mucho m√°s que eso, int√©ntalo hacer mejor esta vez.‚Äù</p>

</blockquote>

<p>De esta forma consigue que Opus pase del autoretrato que muestro a la izquierda al de la derecha (una animaci√≥n de una esfera de puntos).</p>

<div class="image-gallery-embed" data-attrs="{&quot;gallery&quot;:{&quot;images&quot;:[{&quot;type&quot;:&quot;image/png&quot;,&quot;src&quot;:&quot;9826c86e-ba46-4d64-bb18-54898f03a413_714x870.png&quot;},{&quot;type&quot;:&quot;image/png&quot;,&quot;src&quot;:&quot;908f766a-1ba5-40e0-9b01-03572ac5a26a_736x672.png&quot;}],&quot;caption&quot;:&quot;&quot;,&quot;alt&quot;:&quot;&quot;,&quot;staticGalleryImage&quot;:{&quot;type&quot;:&quot;image/png&quot;,&quot;src&quot;:&quot;912eb870-be54-46f4-93ea-65561db91df8_1456x720.png&quot;}},&quot;isEditorNode&quot;:true}">
</div>
<p>Anthropic tiene recursos muy interesantes sobre c√≥mo construir los prompts:</p>

<ul>
<li>
<p>
<a href="https://docs.anthropic.com/claude/docs/prompt-engineering">P√°gina sobre prompt engineering</a> con t√©cnicas y ejemplos.</p>

</li>
<li>
<p>
<a href="https://docs.anthropic.com/claude/prompt-library">Biblioteca de prompts</a>, con ejemplos que van desde c√≥mo generar queries SQL hasta c√≥mo crear poes√≠as, historias con personajes o recetas de cocina.</p>

</li>
</ul>

<p>5Ô∏è‚É£ Recordando a <strong>Akira Toriyama</strong>, que muri√≥ el 8 de marzo, con 68 a√±os de edad, <a href="https://x.com/domingogallardo/status/1765989812824089074?s=20">publiqu√© en X</a> un par de im√°genes de su libro de ilustraciones sobre Dragon Ball (ya agotado). </p>

<p>
<img src="2c8c418c-1bcf-43b2-9408-d8a44d01f9b0_2771x3620.png" alt="">
</p>

<p>La primera imagen muestra un autorretrato y un comentario suyo que indica su nivel de exigencia. Demasiada. Menos mal que en la entrevista contenida en el libro el entrevistador comentaba que Akira se re√≠a mientras contaba algo similar. Y la segunda ilustraci√≥n con una genial galer√≠a de algunos personajes que ya aparec√≠an al comienzo de Dragon Ball.</p>

<div class="image-gallery-embed" data-attrs="{&quot;gallery&quot;:{&quot;images&quot;:[{&quot;type&quot;:&quot;image/jpeg&quot;,&quot;src&quot;:&quot;6d4be1ec-c2c1-4e27-be80-4fb70b02fa58_1536x2048.jpeg&quot;},{&quot;type&quot;:&quot;image/jpeg&quot;,&quot;src&quot;:&quot;c611c52b-76fe-425c-9cf7-d8073d7a3bd3_2048x1536.jpeg&quot;}],&quot;caption&quot;:&quot;&quot;,&quot;alt&quot;:&quot;&quot;,&quot;staticGalleryImage&quot;:{&quot;type&quot;:&quot;image/png&quot;,&quot;src&quot;:&quot;407b3d04-e258-43a8-8d9f-ef83ceef78be_1456x720.png&quot;}},&quot;isEditorNode&quot;:true}">
</div>
<p>Kiko Llaneras <a href="https://x.com/kikollan/status/1766158179447042550?s=20">ha publicado en X</a> un hilo buen√≠simo sobre aquella √©poca en la que segu√≠amos Dragon Ball por la televisi√≥n auton√≥mica. Yo la segu√≠a por TV3,  que se captaba en Alicante gracias a las antenas que instal√≥ Acci√≥ Cultural. En aquella √©poca yo estudiaba Inform√°tica en Valencia, y en casa me grababan la serie para verla los fines de semana que volv√≠a a Alicante. Kiko explica muy bien en el hilo el ansia que ten√≠amos por encontrar m√°s material e informaci√≥n sobre esos dibujos que nos ten√≠an enganchados (entonces no exist√≠a la web, ni Google). Nos ten√≠amos que conformar con fancines de fotocopias que compr√°bamos en la tiendecilla que ten√≠a Ateneo al principio.</p>

<p>Mucho despu√©s ya me compr√© la colecci√≥n completa de tomos de Dragon Ball. Est√°n desgastados por las veces que los hemos le√≠do en la familia.</p>

<p>
<img src="c708e973-7664-4134-b0d3-754d39e8b9b6_4005x1863.png" alt="">
</p>

<p>Akira Toriyama fue un genio y Dragon Ball es genial. Es incre√≠ble la variedad de personajes, el humor, la forma de dibujar la acci√≥n o la originalidad de sus paneles. Y adem√°s es una historia que es un gran culebr√≥n: los personajes evolucionan, tienen hijos, mueren y resucitan. Es un tebeo divertido con una imaginaci√≥n desbordante.</p>

<p>Aqu√≠ dejo algunos ejemplos de vi√±etas.</p>

<div class="image-gallery-embed" data-attrs="{&quot;gallery&quot;:{&quot;images&quot;:[{&quot;type&quot;:&quot;image/jpeg&quot;,&quot;src&quot;:&quot;9e91998e-13de-45d2-9f56-1cfd137632be_1500x2250.jpeg&quot;},{&quot;type&quot;:&quot;image/jpeg&quot;,&quot;src&quot;:&quot;334465d1-8627-4d63-9643-9d161946e20d_602x903.jpeg&quot;}],&quot;caption&quot;:&quot;&quot;,&quot;alt&quot;:&quot;&quot;,&quot;staticGalleryImage&quot;:{&quot;type&quot;:&quot;image/png&quot;,&quot;src&quot;:&quot;5d12c473-c995-4c86-a6bc-102c76ffbc15_1456x720.png&quot;}},&quot;isEditorNode&quot;:true}">
</div>
<div class="image-gallery-embed" data-attrs="{&quot;gallery&quot;:{&quot;images&quot;:[{&quot;type&quot;:&quot;image/jpeg&quot;,&quot;src&quot;:&quot;188c6dc5-4c73-45f8-9323-f945dd35b9ec_800x1200.jpeg&quot;},{&quot;type&quot;:&quot;image/jpeg&quot;,&quot;src&quot;:&quot;49f8c13a-7579-4ce9-9fb4-83d21bf80bdd_800x1200.jpeg&quot;}],&quot;caption&quot;:&quot;&quot;,&quot;alt&quot;:&quot;&quot;,&quot;staticGalleryImage&quot;:{&quot;type&quot;:&quot;image/png&quot;,&quot;src&quot;:&quot;b4c06e26-c9b6-4bef-8bb8-b534f819600c_1456x720.png&quot;}},&quot;isEditorNode&quot;:true}">
</div>
<h2>üë∑‚Äç‚ôÇÔ∏è Mis quince d√≠as</h2>

<h3>üë®‚Äçüíª Trasteando</h3>

<p>Como he comentado antes, una de las cosas que he estado haciendo esta quincena es intentar comprobar de alguna forma objetiva la capacidad de los distintos LLMs. </p>

<p>Una de las cr√≠ticas que se hacen sobre ellos es que no tienen capacidad de planificar, ni tienen un modelo del mundo f√≠sico muy elaborado. Se me ha ocurrido poner a prueba esto, con un test que recuerdo haber hecho la primera vez que apareci√≥ GPT-3.5: hacerle predecir el resultado de mover figuras en un mundo de bloques simplificado. En ese momento comprob√© que, efectivamente, GPT no sab√≠a resolver este tipo de problemas. ¬øQu√© pasa con modelos m√°s avanzados como GPT-4 o Opus?</p>

<p>En concreto, el prompt que he propuesto es el siguiente:</p>

<pre>
<code>Resuelve el siguiente problema:\n\nImagina una columna con los siguientes elementos de arriba a abajo: c√≠rculo, cuadrado, tri√°ngulo.\n\nImagina ahora que la acci√≥n "mueve top a la derecha" coge el elemento que hay arriba de una columna (elimin√°ndolo) y lo coloca a la derecha de la columna, sobre el elemento que hay en la columna de la derecha. En el caso en que en la columna de la derecha no tenga elementos, se coloca en la parte inferior.\n\nDescr√≠beme el resultado final de las siguientes acciones:&nbsp;\n\n1. Mueve top de la columna 1 a la derecha.\n2. Mueve top de la columna 1 a la derecha.\n</code>
</pre>

<p>El problema se puede ir complicando a√±adiendo acciones, e incluyendo la acci√≥n de mover a la izquierda. Por ejemplo, la siguiente figura muestra el resultado de 9 acciones.</p>

<p>
<img src="IMG_1290 2.png" alt="">
</p>

<p>Recordemos que los LLMs son modelos que no tienen ning√∫n tipo de memoria intermedia con la que guardar resultados parciales y por lo tanto no pueden reflexionar sobre ellos ni realizar planes. Imagina intentar resolver este problema de cabeza, sin poder dibujar los resultados intermedios en el papel.</p>

<p>De hecho, en los intentos de resoluci√≥n del problema, los modelos van listando los resultados parciales en la propia conversaci√≥n, como t√©cnica para paliar el problema de la falta de memoria (es algo parecido a la t√©cnica de prompting en la que se le dice al modelo que piense "paso a paso").</p>

<p>Vamos con los resultados. He probado el problema en la consola de Claude y de GPT, y en el interfaz web de Gemini Advanced. He ido probando el problema comenzando con una acci√≥n y a√±adiendo en cada prueba, una a una, m√°s acciones, seg√∫n el dibujo anterior. Los resultado son los siguientes:</p>

<p>
<strong>Con solo una acci√≥n</strong>
</p>

<p>Cuando pedimos que resuelvan el problema solo con la primera acci√≥n todos los modelos resuelven el problema correctamente.</p>

<p>
<strong>Con las dos primeras acciones</strong>
</p>

<p>Cuando complicamos el problema, a√±adiendo una segunda acci√≥n, ya se hace demasiado complicado para:</p>

<ul>
<li>
<p>GPT-3.5</p>

</li>
<li>
<p>Gemini Advanced (Ultra)</p>

</li>
<li>
<p>Claude 2.1</p>

</li>
</ul>

<p>Es sorprendente que el modelo de Google Gemini Ultra, que se promociona como igual que potente que GPT-4, tampoco lo solucione. Algo pasa con el modelo de Google.</p>

<p>Los modelos que lo hacen bien son:</p>

<ul>
<li>
<p>Claude 3 Sonnet</p>

</li>
<li>
<p>GPT-4</p>

</li>
<li>
<p>Claude 3 Opus</p>

</li>
</ul>

<p>Sonnet, pese a ser un modelo comparable en tama√±o a GPT-3.5, lo resuelve bien, al igual que los modelos m√°s potentes.</p>

<p>
<strong>Con una tercera acci√≥n</strong>
</p>

<p>Si a√±adimos la acci√≥n 3:</p>

<pre>
<code>
<code>3. Mueve top de la columna 2 a la derecha</code>
</code>
</pre>

<p>Sonnet deja de hacerlo bien y solo quedan como modelos que resuelven el test correctamente:</p>

<ul>
<li>
<p>GPT-4</p>

</li>
<li>
<p>Claude 3 Opus:</p>

</li>
</ul>

<p>
<strong>¬øCuando dejan de hacerlo bien?</strong>
</p>

<ul>
<li>
<p>GPT-4: Con 6 acciones lo hace siempre bien. Con 7 acciones a veces acierta el resultado y otras no. Con 8 acciones lo hace siempre mal.</p>

</li>
<li>
<p>Claude 3 Opus: Con 4 acciones lo hace bien, pero con 5 ya no.</p>

</li>
</ul>

<p>Se confirman las sensaciones de que Opus y GPT-4 son los modelos m√°s potentes de la actualidad.</p>

<p>Volveremos a probar el test cuando Open AI saque su GPT-4.5 o GPT-5.</p>

<h3>üìñ Un libro</h3>

<p>
<a href="https://www.goodreads.com/review/show/6287789159">Termin√© los Despose√≠dos</a> de Ursula K. Le Guin. 5 estrellas (de 5) para un libro enorme de hace 50 a√±os que explora la tensi√≥n entre la libertad personal y la justicia social. El libro es uno de los pocos que han conseguido simult√°neamente los tres premios m√°s importantes de la ciencia ficci√≥n: el Hugo, el Nebula y el Locus. </p>

<p>Y con raz√≥n. Me ha gustado todo: la trama, el personaje de Shevek y su lucha por desarrollar la teor√≠a f√≠sica de la simultaneidad (y conseguir una sociedad mejor), la relaci√≥n de pareja entre Shevek y Takver, c√≥mo el lenguaje sirve en Urras para establecer valores sociales o la ambientaci√≥n y el detalle con el que se explican las dos sociedades de Urras y Anarres. Y muy chulos los acontecimientos y descubrimientos del final (que no cuento para evitar espoilers).</p>

<p>Un gran libro que podr√≠a convertirse gran miniserie de televisi√≥n. A ver si alguna productora se lanza.</p>

<h3>üì∫ Una serie y una pel√≠cula</h3>

<p>De las series que hemos empezado a ver esta quincena destaco dos: <strong>Expatriadas</strong>, en HBO, y <strong>Shogun</strong>, en Disney. </p>

<p>En Expatriadas la directora <strong>Lulu Wang</strong> nos cuenta una historia en la que nos acercamos a personajes de las distintas clases sociales del Hong Kong de la <a href="https://es.wikipedia.org/wiki/Protestas_en_Hong_Kong_de_2014">revoluci√≥n de los paraguas</a> en 2014. Actrices excelentes como <strong>Nikole Kidman</strong>, la joven <strong>Ji-young Yoo</strong> o la entra√±able <strong>Ruby Ruiz</strong> que hace de ni√±era de los hijos de Kidman.</p>

<p>Y de <strong>Shogun</strong> solo hemos visto el primer cap√≠tulo, pero es suficiente para ver el nivel de esta superproducci√≥n hist√≥rica.</p>

<div class="image-gallery-embed" data-attrs="{&quot;gallery&quot;:{&quot;images&quot;:[{&quot;type&quot;:&quot;image/jpeg&quot;,&quot;src&quot;:&quot;6ccbe3f8-2862-47fa-9724-093e776911af_622x921.jpeg&quot;},{&quot;type&quot;:&quot;image/jpeg&quot;,&quot;src&quot;:&quot;0a1ffb5a-eed5-459a-aa2b-38c87525eb2d_800x1200.jpeg&quot;}],&quot;caption&quot;:&quot;&quot;,&quot;alt&quot;:&quot;&quot;,&quot;staticGalleryImage&quot;:{&quot;type&quot;:&quot;image/png&quot;,&quot;src&quot;:&quot;0382b1a4-9bca-476d-9727-b8e827fe5020_1456x720.png&quot;}},&quot;isEditorNode&quot;:true}">
</div>
<p>Y en cuanto pel√≠cula, <a href="https://letterboxd.com/domingogallardo/films/diary/">he visto</a> (dos veces) la <strong>segunda parte de Dune</strong> de <strong>Villeneuve</strong>. No hay que perd√©rsela, un verdadero espect√°culo visual. Cinco estrellas (de cinco).</p>

<p>
<img src="78d76164-828b-4b29-b689-a7158e1247e8_821x1200.jpeg" alt="">
</p>

<p>¬°Hasta la pr√≥xima quincena, nos leemos! üëãüëã</p>

<p>
</p>
