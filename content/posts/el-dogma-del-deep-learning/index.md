---
title: "El dogma del deep learning"
date: 2024-12-18
draft: false
tags:
  - "newsletter"
---
<p>
</p>

<p>
<img src="Pasted image 20241215092155.png" alt="">
</p>

<h2>El art√≠culo de McCulloch y Pitts</h2>

<p>El art√≠culo de <strong>McCulloch y Pitts</strong> de 1943, <em>
<a href="https://home.csulb.edu/~cwallis/382/readings/482/mccolloch.logical.calculus.ideas.1943.pdf">A Logical Calculus of the Ideas Immanent in Nervous Activity</a>
</em>, constituy√≥ el trabajo fundacional del conexionismo y de las redes neuronales artificiales (ANNs, en ingl√©s). El art√≠culo parte de lo que se conoc√≠a en la √©poca sobre el funcionamiento de las neuronas y establece, de una forma densa y muy matem√°tica (tiene 19 p√°ginas en las que se demuestran hasta diez teoremas) una equivalencia entre el funcionamiento de las neuronas y el de una<strong> red de proposiciones l√≥gicas</strong> en las que las neuronas representan variables que solo pueden tener dos valores ("all-or-none") que los autores identifican con los valores booleanos TRUE o FALSE.</p>

<p>
<img src="Pasted image 20241215092106.png" alt="">
</p>

<p>Imagen del art√≠culo original de McCulloch y Pitts que muestra las neuronas como unidades l√≥gicas.</p>

<p>Cinco a√±os antes, en 1938, <strong>Claude Shannon</strong> hab√≠a publicado en el MIT su tesis de m√°ster <em>
<a href="https://dspace.mit.edu/handle/1721.1/11173">A Symbolic Analysis of Relay and Switching Circuits</a>
</em>, en la que demostraba c√≥mo dise√±ar circuitos el√©ctricos concretos que implementaran operaciones l√≥gicas definidas por el √°lgebra de Boole. Este enfoque pr√°ctico permiti√≥ construir <strong>dispositivos capaces de realizar funciones l√≥gicas</strong> b√°sicas como AND, OR y NOT y fue el punto inicial de la vertiginosa d√©cada de los 40 en la que investigadores como <strong>Von Neumann</strong> o <strong>Alan Turing</strong> dise√±aron los primeros ordenadores digitales.</p>

<p>
<img src="Pasted image 20241215122123.png" alt="">
</p>

<p>Tabla en el art√≠culo de Shannon en la que se muestran las operaciones l√≥gicas realizadas por los circuitos dise√±ados.</p>

<p>Aunque <strong>McCulloch y Pitts</strong> no citaron en su art√≠culo a <strong>Shannon</strong>
<a class="footnote-anchor" data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1" href="#footnote-1" target="_self">1</a>, es bastante probable que se inspiraran en su concepto de <strong>circuitos l√≥gicos</strong>. Si esos circuitos pod√≠an ser la base del funcionamiento de los nuevos computadores digitales que se estaban empezando a desarrollar, ¬øpor qu√© no pod√≠an ser tambi√©n la forma en la que las neuronas funcionaban? En cualquier caso, ambos art√≠culos se convirtieron en fundacionales y sentaron las bases de la revoluci√≥n de los computadores digitales de finales de los a√±os 40 y de la revoluci√≥n actual de la inteligencia artificial. Como curiosidad, ambos modelos estaban basados en el <strong>√°lgebra de Boole</strong>, un sistema te√≥rico propuesto 100 a√±os antes por el matem√°tico ingl√©s <strong>George Boole</strong>. Este es un ejemplo fascinante de c√≥mo un trabajo matem√°tico que en principio solo tiene importancia te√≥rica es la base, un siglo despu√©s, de unos avances tecnol√≥gicos radicales: los computadores digitales y la Inteligencia Artificial.</p>

<p>El modelo de <strong>McCulloch y Pitts</strong> con el tiempo ha sufrido algunas modificaciones, siendo la m√°s importante de ellas el modelo de <strong>Perceptron</strong> introducido por <strong>Frank Rosenblatt</strong> en 1958. En este modelo se a√±ad√≠an <strong>pesos</strong> a las conexiones entre las neuronas y se introduc√≠a un umbral de activaci√≥n, que determinaba si una neurona ‚Äúse activaba‚Äù o no (produciendo un 1 o un 0) en funci√≥n de la suma ponderada de sus entradas. Si la suma superaba el umbral, la neurona se activaba.</p>

<p>En las redes neuronales modernas, este concepto ha evolucionado significativamente. Las funciones de activaci√≥n actuales, como la sigmoide, la tangente hiperb√≥lica (tanh) o la ReLU (Rectified Linear Unit), permiten que las neuronas tomen <strong>valores continuos</strong> en lugar de binarios. Por ejemplo, la funci√≥n sigmoide produce valores en el rango [0, 1], mientras que la tangente hiperb√≥lica los normaliza en el intervalo [-1, 1]. Esta evoluci√≥n ha dotado a las redes neuronales modernas de mayor flexibilidad y capacidad de aprendizaje, facilitando la representaci√≥n de relaciones complejas y la propagaci√≥n eficiente de errores durante el entrenamiento.</p>

<p>Con estas modificaciones, con el descubrimiento de algoritmos eficientes de actualizaci√≥n de los pesos y con la <strong>multiplicaci√≥n exponencial</strong> del n√∫mero de neuronas<a class="footnote-anchor" data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2" href="#footnote-2" target="_self">2</a>, se ha conseguido que las ANNs puedan hacer, en <a href="https://github.com/shun-liang/readable-talks-transcriptions/blob/main/neurips_2024/Vincent%20Weisser%20-%20.%40ilyasut%20full%20talk%20at%20neurips%202024%20pre-training%20as%20we%20know%20it%20will%20end%20and%20what%20comes%20next%20is%20superintelligence%20agentic%2C%20reasons%2C%20understands%20and%20is%20self%20aware.md">palabras de Ilya Sutskever</a> en su charla en la conferencia NIPS 2024, cualquier cosa que un humano pueda hacer de forma intuitiva:</p>

<blockquote>
<p>Pueden hacer cualquier cosa que un ser humano pueda hacer en una fracci√≥n de segundo.</p>

</blockquote>

<h2>El cerebro humano como una enorme red neuronal artificial</h2>

<p>Hace m√°s de 80 a√±os, en las conclusiones de su art√≠culo, <strong>McCulloch y Pitts</strong> realizaban afirmaciones rotundas en las que asimilaban el funcionamiento completo del cerebro al estado de su propuesta red de neuronas l√≥gicas:</p>

<blockquote>
<p>La especificaci√≥n, en un momento dado, de la estimulaci√≥n aferente y de la actividad de todas las neuronas constituyentes, cada una con un comportamiento de ‚Äútodo o nada‚Äù, determina el estado [global del cerebro]. <strong>La especificaci√≥n de la red nerviosa proporciona la ley de conexi√≥n necesaria, mediante la cual se puede calcular, a partir de la descripci√≥n de cualquier estado, el estado sucesivo.</strong>
</p>

<p>
<strong>Cada idea y cada sensaci√≥n</strong> se realiza a trav√©s de la actividad dentro de esa red.</p>

</blockquote>

<p>El nivel de autocomplacencia y falta de humildad es sorprendente. Estas afirmaciones no se planteaban como hip√≥tesis o especulaciones futuras, sino como conclusiones definitivas. Sin embargo, carec√≠an de evidencia experimental, ya que se basaban √∫nicamente en una formulaci√≥n matem√°tica, sin referencias a investigaciones en fisiolog√≠a neuronal o celular que respaldaran tales ideas.</p>

<p>Curiosamente, parece que el tiempo les ha dado la raz√≥n. La noci√≥n de que las ANNs representan una abstracci√≥n v√°lida del funcionamiento de las neuronas naturales se ha consolidado como el <strong>dogma central del conexionismo</strong>, como lo describe <strong>Ilya Sutskever,</strong> quien en la mencionada conferencia NIPS 2024 afirm√≥:</p>

<blockquote>
<p>Es la <strong>idea central del aprendizaje profundo</strong>: la idea del conexionismo. Es la creencia de que, si aceptas que <strong>una neurona artificial es en cierta medida an√°loga a una neurona biol√≥gica</strong>, entonces puedes confiar en que redes neuronales muy grandes ‚Äîaunque no necesariamente tan grandes como el cerebro humano‚Äî pueden configurarse para realizar pr√°cticamente todas las tareas que nosotros, como seres humanos, somos capaces de llevar a cabo.</p>

</blockquote>

<p>
</p>

<p>
<img src="Pasted image 20241216132628.png" alt="">
</p>

<p>Ilya Sutskever en su conferencia plenaria en NIPS 2024, planteando la idea central del conexionismo.</p>

<p>Muchos cient√≠ficos actuales de IA comparten este enfoque reduccionista. Por ejemplo <strong>Oriol Vinyals</strong>, investigador destacado de <strong>Google DeepMind</strong>, declaraba en una entrevista en el <a href="https://youtu.be/78mEYaztGaw?si=LMTItsoqTGAm-DX2">podcast de Deep Mind</a>:</p>

<blockquote>
<p>Puedes imaginar una neurona conectada a varias otras, y lo que haces esencialmente es sumar todas las activaciones de las neuronas entrantes, multiplicadas por sus respectivos pesos. <strong>Es, en esencia, c√≥mo funciona un cerebro</strong>, con cierta libertad creativa.</p>

</blockquote>

<p>Una de las principales ventajas de esta abstracci√≥n ha sido la <strong>eficacia en su computaci√≥n</strong>. Las neuronas y sus pesos se pueden representar mediante enormes matrices de n√∫meros reales, y son procesadas en paralelo por potentes GPUs dise√±adas para realizar c√°lculos masivos de √°lgebra lineal. Esto ha permitido entrenar ANNs cada vez m√°s grandes y complejas, logrando √©xitos espectaculares en reconocimiento de patrones y regularidades, que imitan el desempe√±o de las redes neuronales biol√≥gicas.</p>

<p>Este enfoque reduccionista no es fruto del desconocimiento, sino que es una <strong>decisi√≥n deliberada</strong> de ignorar las complejidades biol√≥gicas que subyacen al comportamiento de las redes neuronales naturales. Por ejemplo, <strong>Demis Hassabis</strong>, tras varios a√±os explorando el funcionamiento biol√≥gico de la inteligencia, considera en su art√≠culo <em>
<a href="https://www.cell.com/neuron/pdf/S0896-6273%2817%2930509-3.pdf">Neuroscience-Inspired Artificial Intelligence</a>
</em> que es una cuesti√≥n de trabajar con el <strong>nivel de abstracci√≥n correcto</strong>, y que √©ste se sit√∫a por encima del sustrato neuronal real:</p>

<blockquote>
<p>Desde un punto de vista pr√°ctico, no necesitamos adherirnos estrictamente a la plausibilidad biol√≥gica para construir sistemas de IA. Lo que funciona es, en √∫ltima instancia, lo que importa. Por lo tanto, <strong>la plausibilidad biol√≥gica es solo una gu√≠a</strong>, no un requisito estricto. Nuestro inter√©s radica en una comprensi√≥n <strong>a nivel algor√≠tmico y computacional</strong> del cerebro, dejando de lado el nivel de implementaci√≥n biol√≥gica.</p>

</blockquote>

<p>
<strong>Hassabis</strong> hace referencia expl√≠cita a los <strong>tres niveles de an√°lisis</strong> propuestos por <strong>David Marr</strong>, y considera que el nivel m√°s bajo, el de los mecanismos f√≠sicos del sustrato neuronal, es menos relevante:</p>

<blockquote>
<p>Nuestro inter√©s radica en los dos niveles superiores de los tres niveles de an√°lisis que <strong>Marr</strong> estableci√≥ para comprender cualquier sistema biol√≥gico complejo: el nivel computacional (el objetivo del sistema y el problema que resuelve) y el nivel algor√≠tmico (los procesos y algoritmos que permiten alcanzar ese objetivo). El nivel de implementaci√≥n, que aborda los <strong>mecanismos f√≠sicos</strong> espec√≠ficos, <strong>es menos relevante</strong> aqu√≠.</p>

</blockquote>

<h2>Los ritmos de las neuronas</h2>

<p>Lo que es poco relevante para los ingenieros e inform√°ticos es el d√≠a a d√≠a de investigaci√≥n de los neurocient√≠ficos. Y la visi√≥n que ellos tienen es completamente distinta.</p>

<p>Por un lado, para <strong>simular de forma te√≥rica el funcionamiento de una neurona</strong>, deben emplear complejas ecuaciones diferenciales que describen la evoluci√≥n temporal de distintas concentraciones de iones a trav√©s de sus membranas celulares, incluyendo los cambios que ocurren en las sinapsis. Hay conexiones que refuerzan el potencial (excitadoras) y otras que lo inhiben (inhibidoras). Ambas interacciones ocurren como <strong>parte de patrones din√°micos de disparo</strong> en redes formadas por <strong>miles de neuronas</strong> conectadas. El resultado de toda esta interacci√≥n es una r√°faga de disparos (<em>
<strong>spikes</strong>
</em>) de la neurona que, a su vez, se convierte en la entrada para muchas otras neuronas.</p>

<p>
<img src="4-Figure1-1.png" alt="">
</p>

<p>Esquema con la secuencia temporal de disparos que entran en una neurona y la secuencia de salida de disparos de la misma neurona.</p>

<p>Por otro lado, los avances experimentales han permitido observar directamente la actividad neuronal gracias a t√©cnicas como el uso de <strong>micro-electrodos</strong>, que pueden registrar tanto la actividad de poblaciones completas como la de neuronas individuales. Estos registros han revelado que las neuronas se disparan en patrones temporales precisos, y que la <strong>frecuencia</strong> de estos disparos puede <strong>codificar informaci√≥n</strong>. Por ejemplo, en neuronas conectadas a m√∫sculos, la intensidad de los disparos puede reflejar la fuerza del movimiento que se requiere, mientras que en otras √°reas del cerebro los patrones temporales est√°n asociados con la transmisi√≥n de se√±ales sensoriales o cognitivas.</p>

<p>
<img src="Pasted image 20241218090922.png" alt="">
</p>

<p>Registro temporal de la actividad de neuronas individuales mientras un rat√≥n se desplaza a lo largo de un entorno lineal de 170 cm. (G) El panel superior muestra las r√°fagas de disparos (<em>spikes</em>) de distintas neuronas (celdas numeradas) en relaci√≥n con la posici√≥n y la velocidad del rat√≥n. Cada punto de color representa un evento de disparo asociado a una neurona espec√≠fica. (H) Los recuadros ampliados (marr√≥n, amarillo y morado) muestran los patrones precisos de disparos en intervalos de 250 ms, destacando la din√°mica temporal y la sincronizaci√≥n entre las neuronas. Este tipo de actividad refleja c√≥mo las redes neuronales biol√≥gicas organizan la informaci√≥n en escalas temporales precisas y exhiben ritmos que facilitan la integraci√≥n de est√≠mulos espaciales y motores. Gy√∂rgy Buzs√°ki y Mih√°ly V√∂r√∂slakos, <em>
<a href="https://www.sciencedirect.com/science/article/pii/S0896627323002143">"Brain rhythms have come of age‚Äù</a>, Neuron, Abril 2023.</em>
</p>

<p>La importancia de los <strong>ritmos del cerebro</strong> es un campo de creciente inter√©s en la neurociencia actual. Adem√°s de los patrones de disparo individuales, se han identificado <strong>oscilaciones r√≠tmicas</strong> en distintas frecuencias, como theta, gamma y delta, que organizan la actividad neuronal en escalas temporales m√°s amplias. </p>

<p>
<strong>Jeff Lichtman</strong>, destacado neurocient√≠fico de Harvard y pionero en el mapeo del conectoma, destac√≥ en una reciente entrevista en el <a href="https://www.preposterousuniverse.com/podcast/2024/12/09/298-jeff-lichtman-on-the-wiring-diagram-of-the-brain/"><em>podcast Mindscape</em></a> de <strong>Sean Carroll</strong> que, aunque los avances en la neurociencia han permitido mapear con gran detalle las conexiones neuronales, estas no pueden capturar la <strong>complejidad din√°mica</strong> del cerebro. Como √©l mismo se√±ala:</p>

<blockquote>
<p>Aunque es posible mapear con gran detalle la red de conexiones neuronales, estos mapas <strong>no revelan</strong> la fuerza de las sinapsis, las <strong>no linealidades</strong> en la respuesta de las c√©lulas y, especialmente, el momento en que los diferentes est√≠mulos activan la c√©lula. Hay tanto entradas excitadoras como inhibidoras, adem√°s de entradas moduladoras de neurotransmisores. Y todo eso es latente, pero no alcanzable, en el diagrama de conexiones.</p>

</blockquote>

<p>Esta dimensi√≥n temporal es un fen√≥meno f√≠sico exclusivo de las redes neuronales biol√≥gicas, que los modelos conexionistas de deep learning no pueden capturar. Su existencia en el cerebro real sugiere que desempe√±an un papel fundamental que a√∫n no comprendemos del todo. Como, seg√∫n <strong>Lichtman</strong>, tampoco comprendemos la complejidad del cerebro:</p>

<blockquote>
<p>Hay ciertas cosas en el mundo, quiz√° el cerebro sea un ejemplo, que no tienen simplificaci√≥n posible. Si la hubiera, los cerebros habr√≠an sido m√°s simples.</p>

</blockquote>

<p>¬øPara qu√© sirven las oscilaciones r√≠tmicas del cerebro? ¬øQu√© codifican? ¬øQu√© importancia tienen los distintos patrones de disparo de las neuronas? </p>

<p>En el pr√≥ximo art√≠culo revisaremos estas preguntas con m√°s detalle y plantear√© mi <strong>especulaci√≥n favorita</strong>: la <strong>consciencia</strong> podr√≠a ser un fen√≥meno emergente de los patrones oscilatorios del cerebro. M√°s espec√≠ficamente, la 'capacidad de sentir' (<em>
<strong>sentience</strong>
</em>, en ingl√©s) ser√≠a el resultado de los patrones de disparo en las redes neuronales biol√≥gicas, presentes en la mayor parte de los <strong>seres vivos con sistemas neuronales complejos</strong>. Sin embargo, esta capacidad resulta <strong>inalcanzable</strong> para los sistemas artificiales basados en procesos secuenciales algor√≠tmicos que, por su propia naturaleza, nunca tendr√°n la capacidad de experimentar sensaciones.</p>

<div>
<hr>

</div>
<p>¬°Hasta la pr√≥xima, nos leemos! üëãüëã</p>

<div class="footnote" data-component-name="FootnoteToDOM">
<a id="footnote-1" href="#footnote-anchor-1" class="footnote-number" contenteditable="false" target="_self">1</a>
<div class="footnote-content">
<p>En su art√≠culo solo hab√≠a tres referencias, una de ellas al <em>Principia Mathematica</em> de <strong>Russell</strong>. Un joven ingeniero de veintipocos a√±os como <strong>Shannon</strong>, que acababa de graduarse con una tesis de m√°ster, no ten√≠a el pedigr√≠ suficiente para aparecer junto a esos pocos gigantes referenciados.</p>

</div>
</div>
<div class="footnote" data-component-name="FootnoteToDOM">
<a id="footnote-2" href="#footnote-anchor-2" class="footnote-number" contenteditable="false" target="_self">2</a>
<div class="footnote-content">
<p>Se ha pasado de los primeros modelos de decenas de neuronas distribuidas en una o dos capas de las ANNs multi-capa de los a√±os 1980 a las decenas de millones de neuronas distribuida en alrededor de 100 capas de GPT-3 en los a√±os 2020.</p>

<p>
</p>

</div>
</div>
