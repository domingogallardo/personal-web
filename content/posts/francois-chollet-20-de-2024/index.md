---
title: "Fran√ßois Chollet (#20 de 2024)"
date: 2024-12-01
draft: true
tags:
  - "newsletter"
---
<p>
<img src="3b65861f-2a0a-4dfa-8317-157eea55d952_1540x590.png" alt="">
</p>

<h2>Entrevista en MLST a Fran√ßois Chollet</h2>

<p>El pasado 6 de noviembre, el podcast <a href="https://creators.spotify.com/pod/show/machinelearningstreettalk">Machine Learning Street Talk</a> public√≥ una interesant√≠sima entrevista con <a href="https://fchollet.com/">Fran√ßois Chollet</a>. Se trata de una conversaci√≥n de m√°s de 2 horas y medias en las que <strong>Chollet</strong> revisa en profundidad temas que van desde aspectos t√©cnicos, como el funcionamiento de los LLMs y su diferencia con la AGI, hasta aspectos filos√≥ficos y sociales de la IA, como la emergencia de la consciencia en los ni√±os o el peligro existencial asociado a la IA.</p>

<p>Ya he hablado por aqu√≠ varias veces de <strong>Chollet</strong>. Por ejemplo en <a href="/posts/del-16-al-30-de-junio-12-de-2024/">este post</a> coment√© las entrevistas con <strong>Dwarkesh Patel</strong> y con <strong>Sean Carroll</strong>. All√≠ vimos que es una persona con una base t√©cnica muy importante (es el creador de la librer√≠a <a href="https://keras.io/">Keras</a> para trabajar con redes neuronales y el autor del libro <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning con Python</a>) cuyas opiniones tienen una fuerte repercusi√≥n en la comunidad de LLMs (por ejemplo, su art√≠culo <a href="https://arxiv.org/abs/1911.01547">On the Measure of Intelligence</a> y la actual competici√≥n <a href="https://arcprize.org/">ARC</a>, de la que tambi√©n hablamos <a href="/posts/del-1-al-15-de-junio-11-de-2024/">aqu√≠</a>).</p>

<p>Hoy quiero presentar con detalle esta √∫ltima entrevista del pasado 6 de noviembre. Adem√°s de en el podcast, tambi√©n se puede ver en YouTube (ya va por m√°s de 40.000 visualizaciones).</p>

<div id="youtube2-JTU8Ha4Jyfc" class="youtube-wrap" data-attrs="{&quot;videoId&quot;:&quot;JTU8Ha4Jyfc&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM">
<div class="youtube-inner">
<iframe src="https://www.youtube-nocookie.com/embed/JTU8Ha4Jyfc?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409">
</iframe>
</div>
</div>
<p>En este art√≠culo voy a extraer y comentar algunas citas de la entrevista. Dada la extensi√≥n de la entrevista, he tenido que hacer una importante selecci√≥n, mostrando las partes que m√°s me han interesado. Se trata, por ello, de un art√≠culo bastante sesgado. Pero no he cambiado en absoluto el sentido de las afirmaciones de <strong>Chollet</strong>. No est√° todo lo que ha dicho, pero s√≠ que ha dicho todo lo que est√°.</p>

<p>Si quer√©is consultar detalles m√°s t√©cnicos sobre los temas tratados aqu√≠, pod√©is revisar la charla que han preparado <strong>Chollet</strong> y <strong>Mike Knoop</strong>, el otro organizador de la <strong>competici√≥n ARC</strong>, para un tour universitario de presentaci√≥n del reto:</p>

<ul>
<li>
<p>
<a href="https://youtu.be/NDbNlPiS898">YouTube</a>
</p>

</li>
<li>
<p>
<a href="https://arcprize.org/blog/beat-arc-agi-deep-learning-and-program-synthesis">Post</a>
</p>

</li>
<li>
<p>
<a href="https://docs.google.com/presentation/d/137ZaRyyxbLsd4QfYZ30HPdaqYrJ8HaKdhzApXYFRoRw/edit?usp=sharing">Diapositivas</a>
</p>

</li>
</ul>

<p>
</p>

<h3>Inteligencia de sistemas 1 y 2</h3>

<p>
<img src="system 1 - system 2.jpg" alt="">
</p>

<p>Imagen generada por ChatGPT.</p>

<p>La teor√≠a de los <strong>sistemas 1 y 2,</strong> propuesta por <strong>Daniel Kahneman</strong> en su libro <a href="https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow">Thinking, Fast and Slow</a>, describe dos modos de pensamiento que operan en la mente humana. El <strong>Sistema 1</strong> es r√°pido, autom√°tico, intuitivo y emocional. Funciona de manera subconsciente y se basa en patrones preexistentes para tomar decisiones sin esfuerzo consciente. Por otro lado, el <strong>Sistema 2</strong> es lento, deliberado, l√≥gico y anal√≠tico. Se activa cuando necesitamos concentrarnos, resolver problemas complejos o tomar decisiones importantes que requieren un an√°lisis cuidadoso.</p>

<p>Esta teor√≠a ha tenido un fuerte impacto en <strong>Chollet</strong>, que cree que los LLMs pueden implementar f√°cilmente Sistema 1, pero no el Sistema 2.</p>

<blockquote>
<p>Los modelos de deep learning son excelentes para producir resultados que son direccionalmente precisos, pero no necesariamente exactos. Son muy buenos para hacer sugerencias √∫tiles. <strong>El proceso del Sistema 1 es similar a lo que hacen los LLMs</strong>. Tambi√©n se basan en pattern matching y mecanismos similares a la intuici√≥n.</p>

</blockquote>

<p>Para <strong>Chollet</strong>, a diferencia del Sistema 1, podemos entender el funcionamiento del Sistema 2 mediante la introspecci√≥n:</p>

<blockquote>
<p>La introspecci√≥n puede ser muy efectiva para comprender c√≥mo tu mente maneja el pensamiento del Sistema 2. No es tan efectiva para el Sistema 1, ya que este opera de manera inconsciente e instant√°nea, en partes del cerebro a las que no tienes acceso directo. Ocurre bajo la superficie, fuera de la observaci√≥n consciente.</p>

<p>El Sistema 2, por otro lado, es deliberado, lento y de baja capacidad. Solo ocurren unas pocas cosas en un momento dado, y es inherentemente introspectivo.</p>

</blockquote>

<p>El funcionamiento de la mente cuando trabaja en modo Sistema 2 es similar a la <strong>ejecuci√≥n paso a paso de un programa</strong>. Es lo que hacemos cuando, por ejemplo, ordenamos una lista de n√∫meros o sumamos mentalmente dos n√∫meros largos: ejecutamos un algoritmo que hemos aprendido.</p>

<blockquote>
<p>Tus pensamientos existen en tu mente en forma de programas.</p>

</blockquote>

<p>
<strong>Chollet</strong> argumenta que una caracter√≠stica fundamental de la inteligencia es la capacidad de ejecutar mentalmente estos programas, verificar si funcionan correctamente y resolver una tarea nueva seleccionando los mejores y combin√°ndolos:</p>

<blockquote>
<p>Cuando te enfrentas una tarea nueva, puedes describirla mentalmente utilizando un conjunto de propiedades y luego generar un peque√±o n√∫mero de hip√≥tesis sobre programas que cumplan con esas restricciones descriptivas. Despu√©s, <strong>pruebas mentalmente estas hip√≥tesis</strong> para verificar si tu intuici√≥n es correcta. Ese es un ejemplo cl√°sico del pensamiento del Sistema 2: es, esencialmente, c√≥mo funciona la <strong>s√≠ntesis de programas</strong> en el cerebro.</p>

</blockquote>

<h3>Las limitaciones del deep learning</h3>

<p>
<img src="Captura de pantalla 2024-12-01 a las 7.26.56.png" alt="">
</p>

<p>Diapositiva del ARC Prize 2024 University Tour.</p>

<p>Cuando <strong>Chollet</strong> comenz√≥ a trabajar con las redes neuronales profundas, pensaba que ser√≠an tan potentes como <strong>m√°quinas de Turing</strong> y podr√≠an implementar algoritmos gen√©ricos. Despu√©s de trabajar con ellas varios a√±os intentado usarlas como demostradores de teoremas, se dio cuenta de que su funcionamiento estaba basado en reconocimiento de patrones.</p>

<p>Al principio, <strong>Chollet</strong> cre√≠a que el <em>deep learning</em> pod√≠a conseguir cualquier cosa:</p>

<blockquote>
<p>Yo, como muchos otros en el campo, asum√≠a que los modelos de aprendizaje profundo eran un sustrato computacional general, capaz de realizar cualquier tipo de c√°lculo. Cre√≠a que eran <strong>completos en el sentido de Turing</strong>. En aquel entonces, entre 2015 y 2016, se discut√≠an ampliamente ideas similares, como el concepto de m√°quinas de Turing neuronales. Hab√≠a un sentimiento de optimismo de que el <em>deep learning</em> podr√≠a eventualmente reemplazar por completo el software escrito a mano, y al principio me adher√≠ a esa visi√≥n.</p>

</blockquote>

<p>Pero llegaron los problemas, cuando intent√≥ usar <em>deep learning</em> para demostraci√≥n autom√°tica de teoremas. Las redes neuronales solo trabajaban por reconocimiento de patrones, no eran capaces de realizar programas secuenciales discretos:</p>

<blockquote>
<p>Trabajaba en demostraci√≥n autom√°tica de teoremas utilizando aprendizaje profundo junto con <strong>Christian Szegedy</strong>. La idea clave detr√°s de este trabajo era que la demostraci√≥n de teoremas es similar a la s√≠ntesis de programas, ya que implica un proceso de b√∫squeda en √°rbol guiado por operadores y axiomas. Nuestro objetivo era utilizar un modelo de <em>deep learning</em> para guiar esa b√∫squeda.</p>

<p>Dediqu√© una cantidad significativa de tiempo a explorar este enfoque, probando muchas ideas diferentes. Aunque los resultados eran mejores que al azar, un an√°lisis m√°s profundo revel√≥ que las mejoras proven√≠an del reconocimiento superficial de patrones, m√°s que de un razonamiento aut√©ntico del Sistema 2. <strong>Los modelos no estaban aprendiendo programas generales y discretos</strong>; simplemente explotaban un atajo basado en el reconocimiento de patrones que siempre estaba disponible. Esta realizaci√≥n me pareci√≥ un gran obst√°culo. No importaba cu√°nto ajustara la arquitectura, los datos de entrenamiento u otros elementos, los modelos siempre tend√≠an a recurrir a estos atajos.</p>

</blockquote>

<p>El reconocimiento de patrones no era suficiente para hacer deducciones autom√°ticas. <strong>Chollet</strong> concluy√≥ que para eso es necesario sintentizar algoritmos discretos:</p>

<blockquote>
<p>Este fue un punto de inflexi√≥n para m√≠. Estos modelos eran, en esencia, motores de reconocimiento de patrones. Para alcanzar un razonamiento propio del Sistema 2, se necesitaba algo m√°s: la <strong>s√≠ntesis de programas</strong>.</p>

</blockquote>

<div>
<hr>

</div>
<p>
<strong>Inciso especulativo: Srinivasa Ramanujan ¬øun genio matem√°tico del Sistema 1?</strong>
</p>

<p>Las ideas de <strong>Chollet</strong> sobre la necesidad del razonamiento de Sistema 2 y las limitaciones del Sistema 1 son compartidas por casi toda la comunidad. Sin embargo, se me plantea una duda: ¬øcu√°les son los l√≠mites del reconocimiento de patrones? ¬øSe puede crear un sistema intuitivo que genere teoremas matem√°ticos? Tenemos un caso en el que parece que s√≠: el matem√°tico indio <strong>Srinivasa Ramanujan</strong>.</p>

<p>
<img src="Srinivasa_Ramanujan_-_OPC_-_2_(cleaned).jpg" alt="">
</p>

<p>El genial matem√°tico ind√∫ Srinivasa Ramanujan.</p>

<p>
<strong>Srinivasa Ramanujan</strong> (1887-1920) fue un genio autodidacta que creci√≥ en la India y, a pesar de tener acceso limitado a educaci√≥n formal en matem√°ticas avanzadas, logr√≥ desarrollar resultados asombrosos en √°reas como teor√≠a de n√∫meros, fracciones continuas y series infinitas.</p>

<p>Seg√∫n quienes trabajaron con √©l, como el matem√°tico brit√°nico <strong>G. H. Hardy</strong>, Ramanujan obten√≠a resultados de una manera profundamente intuitiva, casi como si ‚Äúaparecieran‚Äù en su mente. A menudo presentaba f√≥rmulas y teoremas directamente, sin proporcionar demostraciones formales o los pasos intermedios tradicionales.</p>

<p>Ramanujan describ√≠a que sus intuiciones matem√°ticas surg√≠an como una especie de inspiraci√≥n divina, y atribu√≠a su capacidad a la diosa hind√∫ <strong>Namagiri</strong>, a quien consideraba su gu√≠a espiritual. Por ejemplo, muchas de sus f√≥rmulas sobre fracciones continuas, series infinitas y funciones el√≠pticas parecen haber sido ‚Äúintuidas‚Äù sin recurrir a herramientas convencionales de c√°lculo o deducci√≥n matem√°tica paso a paso.</p>

<p>Quiz√°s esta diosa no era m√°s que la enorme capacidad de <strong>reconocimiento de patrones</strong> que hab√≠a en la mente de <strong>Ramanujan</strong>, despu√©s de haberse entrenado con un vasto n√∫mero de deducciones<a class="footnote-anchor" data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1" href="#footnote-1" target="_self">1</a>. </p>

<div>
<hr>

</div>
<h3>Combinar Sistema 1 y Sistema 2</h3>

<p>
<img src="combinacion-s1-s2.png" alt="">
</p>

<p>Imagen generada por ChatGPT.</p>

<p>
<strong>Chollet</strong> piensa que, en nuestra mente, el Sistema 1 y el Sistema 2 <strong>funcionan simult√°neamente</strong>. La intuici√≥n le ense√±a caminos a la deducci√≥n y desecha opciones que no parecen razonables.</p>

<blockquote>
<p>Es importante recordar que el Sistema 2 no funciona de manera aislada. Siempre hay un componente del Sistema 1 que lo respalda. Estoy convencido de que ning√∫n proceso cognitivo en la mente humana es puramente del Sistema 1 o del Sistema 2. Todo es una mezcla de ambos. Incluso en tareas que parecen muy centradas en el razonamiento, como resolver ARC, hacer matem√°ticas o jugar al ajedrez, hay una cantidad significativa de reconocimiento de patrones e intuici√≥n involucrada.</p>

<p>Por ejemplo, al resolver una tarea de ARC, podr√≠as considerar solo dos o cuatro hip√≥tesis, a pesar del inmenso espacio de posibles programas, que podr√≠a incluir cientos de miles. ¬øQu√© <strong>reduce este espacio a solo unas pocas opciones viables</strong>? La intuici√≥n o el reconocimiento de patrones, que es el trabajo del Sistema 1.</p>

</blockquote>

<p>De hecho, en otra parte de la entrevista, enfatiza que los LLMs funcionan muy bien para realizar b√∫squedas intuitivas en grandes espacios combinatoriales:</p>

<blockquote>
<p>Este proceso es similar a lo que hacen los LLMs. Tambi√©n se basan en mecanismos de <em>pattern matching</em> y una especie de intuici√≥n para explorar espacios vastos y reducirlos a un n√∫mero manejable de posibilidades. Aunque a√∫n es necesario verificar sus resultados, sus conjeturas suelen ser sorprendentemente precisas. Creo que este proceso de reducci√≥n es un aspecto fundamental de la propia cognici√≥n.</p>

</blockquote>

<p>Esto puede ser un camino para construir sistemas m√°s avanzados, combinando un LLM con un <strong>verificador externo</strong>.</p>

<blockquote>
<p>Por eso combinar un LLM con un verificador externo es tan poderoso. Ayuda a navegar el problema de la explosi√≥n combinatoria de probar cada posible soluci√≥n y, al mismo tiempo, compensa las limitaciones de los LLMs, que se basan principalmente en el pensamiento del Sistema 1. Con un verificador externo, se a√±ade una capa de razonamiento del Sistema 2 para el paso cr√≠tico de verificaci√≥n final, asegurando que la soluci√≥n definitiva sea robusta y confiable.</p>

</blockquote>

<p>Esto se parece a lo que puede estar haciendo <strong>o1</strong> en su fase de inferencia, solo que en el caso de <strong>o1</strong> el verificador externo es tambi√©n un LLM. </p>

<p>Por ejemplo, esta fue la forma en la que <strong>DeepMind</strong> program√≥ <a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf">AlphaGo</a>, su sistema para jugar al Go a nivel s√∫perhumano. Utiliz√≥ una combinaci√≥n de redes neuronales para realizar predicciones r√°pidas y heur√≠sticas (basadas en un proceso similar al pensamiento del Sistema 1) y un proceso de b√∫squeda en <strong>√°rboles de Monte Carlo</strong> para evaluar y verificar de manera m√°s exhaustiva las jugadas (un enfoque propio del Sistema 2). Las redes neuronales predec√≠an las mejores jugadas posibles y estimaban la probabilidad de ganar desde una posici√≥n determinada, mientras que el sistema de b√∫squeda verificaba estas jugadas en profundidad, explorando las ramificaciones m√°s prometedoras. Este dise√±o permiti√≥ a <strong>AlphaGo</strong> combinar la intuici√≥n de patrones con un an√°lisis meticuloso, demostrando c√≥mo la interacci√≥n entre procesos similares a los Sistemas 1 y 2 puede resolver problemas de alta complejidad combinatoria de manera eficiente.</p>

<h3>Interpolaci√≥n con abstracciones centradas en valores</h3>

<p>
<img src="Captura de pantalla 2024-11-30 a las 17.19.29.png" alt="">
</p>

<p>Ilustraci√≥n de una mezcla de tres gausianas en un espacio de dos dimensiones. Extra√≠da del libro "<em>Deep Learning - Foundations and Concepts</em>" de Christopher M. Bishop.</p>

<p>En la entrevista, <strong>Chollet</strong> menciona el concepto de abstracciones centradas en valores (<em>value-centric abstractions</em>) cuando est√° discutiendo las limitaciones de las t√©cnicas usadas por el <em>deep learning</em>, ajuste de curvas o descenso de gradiente. Espec√≠ficamente, aparece en esta cita:</p>

<blockquote>
<p>Creo que ajustar curvas param√©tricas o utilizar descenso por gradiente funciona bien para lo que llamo abstracci√≥n centrada en valores (<em>value-centric abstraction</em>). Esta idea se basa en comparar elementos utilizando una <strong>distancia continua</strong>, lo que conduce naturalmente a incrustar estos ‚Äúelementos‚Äù (como im√°genes, conceptos discretos o palabras) en un <em>manifold</em>. En este <em>manifold</em>, los elementos similares se colocan cerca unos de otros, y las diferentes dimensiones de variaci√≥n dentro del espacio adquieren un significado sem√°ntico."</p>

</blockquote>

<p>
<strong>Chollet</strong> contrasta este concepto con la "abstracci√≥n centrada en programas" (<em>program-centric abstraction</em>), explicando que mientras la abstracci√≥n centrada en valores funciona bien con distancias continuas y similitudes, no es adecuada para trabajar con grafos y programas. Como √©l explica:</p>

<blockquote>
<p>Las curvas son adecuadas para este tipo de abstracci√≥n porque fomentan de manera inherente las comparaciones basadas en distancias continuas.</p>

</blockquote>

<p>Estas funciones son creadas por el LLM en el proceso de aprendizaje, como una forma de conseguir predecir el siguiente token. As√≠, el LLM hace algo m√°s que memorizar, es capaz de aprender este tipo de curvas o funciones:</p>

<blockquote>
<p>Los LLMs est√°n entrenados para predecir el siguiente token utilizando modelos altamente flexibles y ricos. En teor√≠a, si tuvieran una capacidad de memoria infinita, podr√≠an actuar como una enorme tabla de b√∫squeda. Sin embargo, en la pr√°ctica, los LLMs est√°n limitados, por tener solo miles de millones de par√°metros. Esta limitaci√≥n los obliga a <strong>comprimir la informaci√≥n</strong> que aprenden, en lugar de memorizar cada secuencia en los datos de entrenamiento. Lo que realmente est√°n aprendiendo son <strong>funciones predictivas</strong>, que toman la forma de funciones vectoriales, ya que, fundamentalmente, los LLMs operan sobre vectores. [...] Estas funciones pueden generalizar de alguna forma los datos de entrenamiento.</p>

</blockquote>

<p>Y cuando preguntamos a un LLM, √©ste es capaz de interpolar estas funciones, combinarlas y componerlas:</p>

<blockquote>
<p>Cuando haces una consulta a un LLM, esencialmente est√°s consultando un punto en el espacio de funciones. Puedes pensar en el LLM como un <em>manifold</em> donde cada punto codifica una funci√≥n. Adem√°s, puedes <strong>interpolar</strong> a trav√©s de este <em>manifold</em> para componer o combinar funciones, lo que te ofrece un n√∫mero infinito de programas potenciales entre los que elegir.</p>

</blockquote>

<p>Art√≠culos recientes, como <a href="https://arxiv.org/abs/2410.21272">Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics</a>, abundan en estas ideas, y muestran c√≥mo los LLMs son capaces de resolver problemas de matem√°ticas usando heur√≠sticas resultantes de la combinaci√≥n de reconocimiento de patrones.</p>

<p>
<strong>Chollet</strong> reconoce que estas funciones aprendidas en el pre-training pueden tener un alto nivel de abstracci√≥n, y capturar elementos abstractos del lenguaje, como el estilo literario de <strong>Shakespeare</strong>:</p>

<blockquote>
<p>Por ejemplo, imagina un LLM encontrando por primera vez las obras de <strong>Shakespeare</strong>. Si ya ha aprendido un modelo general del idioma ingl√©s, puede reutilizar gran parte de ese conocimiento para entender a Shakespeare. El texto puede ser ligeramente diferente, pero a√∫n se ajusta a la misma estructura subyacente del ingl√©s. El modelo puede entonces aprender una funci√≥n de transferencia de estilo que <strong>adapta su comprensi√≥n general del ingl√©s para generar texto al estilo de Shakespeare</strong>. Por eso los LLMs son capaces de realizar tareas como la transferencia de estilo textual.</p>

</blockquote>

<h3>Competici√≥n ARC y la capacidad de la inteligencia de gestionar situaciones novedosas</h3>

<p>
<img src="9a4b2f4c-be66-4736-b28b-f33e642c28a2_1484x636.webp" alt="">
</p>

<p>Ejemplo de tareas a resolver en el test ARC.</p>

<p>
<strong>Chollet</strong> considera que una de las caracter√≠sticas principales de la inteligencia humana es su capacidad de gestionar <strong>situaciones novedosas</strong> y hacerlo con muy pocas muestras:</p>

<blockquote>
<p>Si deseas medir la inteligencia, necesitas evaluar qu√© tan eficientemente el sistema adquiere nuevas habilidades con una cantidad limitada de datos.</p>

</blockquote>

<p>La forma de gestionar situaciones novedosas es mediante la creaci√≥n de nuevas habilidades.</p>

<blockquote>
<p>La inteligencia no es solo una habilidad; es una meta-habilidad, la capacidad con la que <strong>adquieres nuevas habilidades</strong>. La inteligencia es, en esencia, la eficiencia en la adquisici√≥n de habilidades.</p>

</blockquote>

<p>Y, lo m√°s importante, estas nuevas habilidades deben crearse en tiempo real. En el caso de los LLMs, estas nuevas habilidades deber√≠an crearse en lo que se denomina <strong>tiempo de inferencia</strong>, no en el tiempo del pre-entrenamiento. Y esto es algo que los LLMs no pueden hacer:</p>

<blockquote>
<p>Si les pides que resuelvan problemas significativamente diferentes a cualquiera de los que se encuentran en sus datos de entrenamiento, generalmente fallar√°n.</p>

</blockquote>

<p>La capacidad de medir c√≥mo enfrentarse a la novedad es, por tanto, uno de los objetivos principales del test ARC:</p>

<blockquote>
<p>Si deseas medir la inteligencia, necesitas un tipo de prueba diferente, una que no pueda ser superada mediante preparaci√≥n previa. Por ejemplo, ARC es una prueba de este tipo.</p>

<p>GPT-3.5, cuando se utiliza con direct prompting, alcanza aproximadamente un 21% de precisi√≥n en ARC. Esto implica que alrededor del 80% del conjunto de datos es genuinamente novedoso, incluso en comparaci√≥n con la totalidad de internet. Eso es una buena se√±al de la solidez del <em>benchmark</em>.</p>

</blockquote>

<p>Para resolver una tarea de <a href="/posts/del-1-al-15-de-junio-11-de-2024/">ARC</a> debemos construir transformaciones (programas) que convierten una imagen de entrada en una imagen de salida. En estas transformaciones usamos conceptos aprendidos previamente (n√∫mero, posici√≥n, color, etc.). Es como usar bloques de construcci√≥n previos y combinarlos. Pero para poder hacer esto necesitamos ser capaces de ejecutar mentalmente las pruebas y comprobaciones, esta es la habilidad de la que carecen los LLMs:</p>

<blockquote>
<p>En cada tarea del ARC, se te proporcionan de dos a cuatro ejemplos de demostraci√≥n, cada uno compuesto por una imagen de entrada y una de salida. Tu trabajo consiste en identificar la transformaci√≥n o el programa que conecta la entrada con la salida. Despu√©s de aprender este programa a partir de los ejemplos, se te da una nueva cuadr√≠cula de entrada y debes producir la cuadr√≠cula de salida correspondiente para demostrar tu comprensi√≥n.</p>

<p>El principal cuello de botella aqu√≠ es la <strong>explosi√≥n combinatoria</strong> del espacio de programas. El n√∫mero de programas posibles crece exponencialmente con el n√∫mero de bloques de construcci√≥n y el tama√±o del programa. Si buscas programas que involucren, por ejemplo, 40 llamadas a funciones, el espacio se vuelve astron√≥micamente grande, lo que hace imposible iterar exhaustivamente por todas las opciones.</p>

<p>Sin embargo, los humanos no enfrentan este problema de la misma manera. Cuando abordas una tarea de ARC, ejecutas solo un peque√±o n√∫mero de programas paso a paso, principalmente para verificar su correcci√≥n. Este proceso se basa en una forma extremadamente poderosa de <strong>intuici√≥n</strong>, que reduce significativamente el espacio de b√∫squeda. Esa intuici√≥n no es completamente confiable‚Äîpor eso necesitas realizar verificaciones‚Äîpero es direccionalmente correcta. Te orienta hacia posibilidades prometedoras en lo que de otro modo ser√≠a un espacio abrumador de opciones.</p>

</blockquote>

<p>La pr√≥xima semana del 10 al 15 de diciembre, en el <strong>NeurIPS 2024</strong>, se presentar√°n oficialmente los ganadores de la competici√≥n de este a√±o y se dar√°n los premios de $50k a los mejores 5 equipos y de $75k a los 3 mejores papers conceptuales. Tambi√©n se publicar√° un paper resumiendo los mejores avances y el c√≥digo fuente de los avances m√°s importantes. La competici√≥n ya se ha cerrado y no se ha conseguido el premio de $600k al que consiga resolver un 85% de los tests, pero ha habido grandes avances y <strong>los dos mejores equipos han conseguido el 55,5% y el 53,5%</strong>. Comentaremos m√°s detalles por aqu√≠.</p>

<h3>AGI</h3>

<p>
<img src="agi.png" alt="">
</p>

<p>Imagen generada por ChatGPT.</p>

<p>Para terminar, vamos a revisar las opiniones de <strong>Chollet</strong> sobre AGI, bastante optimistas. Resalto especialmente la <strong>separaci√≥n entre AGI y agencia</strong>. Una AGI no lleva asociada el establecimiento de objetivos. Para <strong>Chollet</strong>, eso es algo externo a la propia AGI. Comparto totalmente esta opini√≥n.</p>

<p>La AGI se conseguir√°, pero ser√° solo una herramienta:</p>

<blockquote>
<p>Para m√≠, construir una AGI es un esfuerzo cient√≠fico, y una vez desarrollada, ser√° una herramienta altamente √∫til, nada m√°s. La AGI ser√°, como he dicho antes, un algoritmo de b√∫squeda de caminos para navegar por los espacios de situaciones futuras. Tomar√° informaci√≥n sobre un problema, sintetizar√° un modelo de ese problema y ayudar√° a tomar decisiones basadas en ese modelo. <strong>Ser√° una herramienta valiosa, pero no convertir√° a nadie en un dios.</strong>
</p>

</blockquote>

<p>Por eso, como que cualquier otra herramienta, la AGI no ser√° capaz de tomar decisiones por si misma. No tendr√° objetivos ni agencia:</p>

<blockquote>
<p>
<strong>La inteligencia es distinta de la agencia y la definici√≥n de objetivos.</strong> Si tienes inteligencia de manera aislada, lo √∫nico que tienes es un mecanismo para convertir informaci√≥n en modelos accionables. No es autodirigida ni tiene la capacidad de establecer sus propios objetivos. La definici√≥n de objetivos debe ser un componente externo que se a√±ada de forma deliberada.</p>

<p>En este contexto, la inteligencia es como un algoritmo de b√∫squeda de caminos. Toma el modelo del mundo y el objetivo‚Äîambos proporcionados externamente‚Äîy determina la secuencia correcta de acciones para alcanzar ese objetivo. La inteligencia, en este sentido, se trata de navegar por el ‚Äúespacio de situaciones futuras‚Äù. Es, esencialmente, una b√∫squeda de caminos dentro de ese espacio.</p>

</blockquote>

<p>La agencia, la persecuci√≥n de objetivos, es lo que puede ser peligroso. Pero eso habr√≠a que incluirlo de forma expl√≠cita en el sistema. Construir esa combinaci√≥n s√≠ que podr√≠a tener riesgos:</p>

<blockquote>
<p>Podr√≠as imaginar combinar una AGI‚Äîeste ‚Äògermen‚Äô de inteligencia‚Äîcon un sistema aut√≥nomo de establecimiento de objetivos y un sistema de valores, convirti√©ndola efectivamente en un agente, y luego darle acceso al mundo real. ¬øSer√≠a eso peligroso? S√≠, absolutamente. Pero, en ese caso, habr√°s dise√±ado deliberadamente ese peligro. <strong>No es un riesgo inherente a la AGI en s√≠ misma; es algo que has construido conscientemente.</strong>
</p>

</blockquote>

<p>Pero no se va a llegar a la AGI de forma abrupta y dar√° tiempo a reflexionar sobre ese tipo de riesgos:</p>

<blockquote>
<p>Creo que, una vez que tengamos AGI, tendremos tiempo suficiente para anticipar y mitigar este tipo de riesgos. La AGI ser√° una tecnolog√≠a poderosa, y precisamente por eso ser√° valiosa y √∫til. Cualquier cosa poderosa lleva inherentemente cierto riesgo, pero mantendremos el control porque la AGI, por s√≠ sola, no puede establecer objetivos. Eso solo cambiar√° si deliberadamente creas un mecanismo aut√≥nomo para definir objetivos.</p>

</blockquote>

<p>Terminamos el art√≠culo con esta visi√≥n optimista del futuro. Un futuro que el propio Chollet quiere construir de forma m√°s activa todav√≠a a como lo ha hecho hasta ahora: el pasado 14 de noviembre <a href="https://x.com/fchollet/status/1857012265024696494">anunci√≥ en X</a> que dejaba Google para montar una empresa con una amigo. </p>

<p>
<img src="04585b46-2ea2-4219-81f9-bd49fedda502_1110x256.png" alt="">
</p>

<p>¬°Buena suerte Fran√ßois!</p>

<div>
<hr>

</div>
<p>¬°Hasta la pr√≥xima, nos leemos! üëãüëã</p>

<div class="footnote" data-component-name="FootnoteToDOM">
<a id="footnote-1" href="#footnote-anchor-1" class="footnote-number" contenteditable="false" target="_self">1</a>
<div class="footnote-content">
<p>Quiz√°s si entrenamos un modelo de lenguaje con secuencias completas de deducciones, la red neuronal aprende a identificar patrones usados en estas deducciones y es capaz de generar deducciones que pueden ser correctas. Algo as√≠ es parte de <a href="/posts/como-funciona-o1-15-de-2024/">lo que hace o1</a> o el nuevo modelo abierto chino que intenta imitarlo, <a href="https://qwenlm.github.io/blog/qwq-32b-preview/">Qwen QwQ</a>. Y quiz√°s un modelo m√°s grande, el pr√≥ximo GPT-5, o GPT-6, podr√°n encontrar patrones m√°s complejos de cuando sean entrenados de forma similar.</p>

<p>
</p>

</div>
</div>
